{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity matrix:\n",
      "tensor([[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input texts\n",
    "text1 = \"this is the first text.\"\n",
    "text2 = \"he is a eggplant\"\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokens = tokenizer([text1, text2], padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Generate the BERT embeddings\n",
    "outputs = model(**tokens)\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Calculate similarity using cosine similarity\n",
    "similarity = cosine_similarity(embeddings, embeddings, dim=1)\n",
    "\n",
    "print(f\"Similarity matrix:\\n{similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9761,  0.8488,  0.8985,  0.9620,  0.9672,  0.9361,  0.9398,  0.9939,\n",
      "          0.9560,  0.8898,  0.9228,  0.9933,  0.9483,  0.6590,  0.7932,  0.9931,\n",
      "          0.9688,  0.4430,  0.9622,  0.7619,  0.8228,  0.8193,  0.9656,  0.7524,\n",
      "          0.9957,  0.7742,  0.9358,  0.4830,  0.7489,  0.9480,  0.9630,  0.8613,\n",
      "          0.9103,  0.6640,  0.7027,  0.9062,  0.9872,  0.9091,  0.9371,  0.9601,\n",
      "          0.9855,  0.9413,  0.9488,  0.9107,  0.9941,  0.9442,  0.9678,  0.9032,\n",
      "          0.8676,  0.9703,  0.9815,  0.8849,  0.9315,  0.9265,  0.9906,  0.9809,\n",
      "          0.9113,  0.9776,  0.9409,  0.3558,  0.9711,  0.9530,  0.8570,  0.9202,\n",
      "          0.9499,  0.7722,  0.9653,  0.9917,  0.9937,  0.9363,  0.9727,  0.9850,\n",
      "          0.8450,  0.8572,  0.9257,  0.8860,  0.9347,  0.9764,  0.9852,  0.9748,\n",
      "          0.9712,  0.9924,  0.8240,  0.9288,  0.9596,  0.7782,  0.9809,  0.8212,\n",
      "          0.8698,  0.9802,  0.8960,  0.9630,  0.8945,  0.7831,  0.7525,  0.9963,\n",
      "          0.4533,  0.6399,  0.9813,  0.9790,  0.5746,  0.9088,  0.9695,  0.8559,\n",
      "          0.8547,  0.8853,  0.8086,  0.9631,  0.9573,  0.9906,  0.9636,  0.9509,\n",
      "          0.7540,  0.8055,  0.9769,  0.9825,  0.7953,  0.9388,  0.8760,  0.9733,\n",
      "          0.8602,  0.9760,  0.9822,  0.9971,  0.9817,  0.9328,  0.8457,  0.9627,\n",
      "          0.8731,  0.9089,  0.8259,  0.9592,  0.9909,  0.9863,  0.8004,  0.8927,\n",
      "          0.9289,  0.9549,  0.9829,  0.9152,  0.9790,  0.9680,  0.9856,  0.6791,\n",
      "          0.8834,  0.9766,  0.9873,  0.9139,  0.9891,  0.9909,  0.7789,  0.9730,\n",
      "          0.9020,  0.7350,  0.9835,  0.9286,  0.8322,  0.9875,  0.8347,  0.9378,\n",
      "          0.9618,  0.8611,  0.9470,  0.9852,  0.4398,  0.7548,  0.9251,  0.9148,\n",
      "          0.7546,  0.9873,  0.9811,  0.9466,  0.9124,  0.8558,  0.9813,  0.9119,\n",
      "          0.9321,  0.9782,  0.9495,  0.9856,  0.9996,  0.9912,  0.9025,  0.9705,\n",
      "          0.9013,  0.9868,  0.9517,  0.5964,  0.9752,  0.9590,  0.7843,  0.9627,\n",
      "          0.9246,  0.9269,  0.9145,  0.8514,  0.8694,  0.9728,  0.9495,  0.7591,\n",
      "          0.9161,  0.8643,  0.9807,  0.9208,  0.9328,  0.9959,  0.9429,  0.7399,\n",
      "          0.7719,  0.9260,  0.9771,  0.9892,  0.8519,  0.9101,  0.9611,  0.6790,\n",
      "          0.9235,  0.8088,  0.9696,  0.9157,  0.9816,  0.8785,  0.9694,  0.6967,\n",
      "          0.9568,  0.9919,  0.9139,  0.9627,  0.9213,  0.9753,  0.9260,  0.5808,\n",
      "          0.9854,  0.9240,  0.9427,  0.9692,  0.9468,  0.9715,  0.7346,  0.8984,\n",
      "          0.8791,  0.7953,  0.9686,  0.9864,  0.9521,  0.9232,  0.9246,  0.9815,\n",
      "          0.8996,  0.9707,  0.9151,  0.8566,  0.3116,  0.9962,  0.9722,  0.9206,\n",
      "          0.9453,  0.9513,  0.8654,  0.9316,  0.6584,  0.8413,  0.7142,  0.4359,\n",
      "          0.9909,  0.9283,  0.7029,  0.7919,  0.9212,  0.9047,  0.9676,  0.6087,\n",
      "          0.9685,  0.9821,  0.9896,  0.9092,  0.9922,  0.9654,  0.9198,  0.9255,\n",
      "          0.9540,  0.9558,  0.9256,  0.9858,  0.9703,  0.9712,  0.9681,  0.9922,\n",
      "          0.9445,  0.9794,  0.9693,  0.8912,  0.7631,  0.9962,  0.9731,  0.8985,\n",
      "          0.9757,  0.7192,  0.9129,  0.9303,  0.8918,  0.8029,  0.9084,  0.9584,\n",
      "          0.9638,  0.9911,  0.8897,  0.9263,  0.9941,  0.9375,  0.9811,  0.8387,\n",
      "          0.8757,  0.9567,  0.9716,  0.9929,  0.9717,  0.2463,  0.9601,  0.9952,\n",
      "          0.9836,  0.9307,  0.8338,  0.9149,  0.9108,  0.9101,  0.9797,  0.9780,\n",
      "          0.9025,  0.9791,  0.8844,  0.9872,  0.9331,  0.9781,  0.9844,  0.9787,\n",
      "          0.9794,  0.9891,  0.9620,  0.7619,  0.9864,  0.9903,  0.9839,  0.8052,\n",
      "          0.9556,  0.8993,  0.9673,  0.9489,  0.9886,  0.9316,  0.8746,  0.8109,\n",
      "          0.9622,  0.6729,  0.9187,  0.9767,  0.5597,  0.9834,  0.9764,  0.9579,\n",
      "          0.9819,  0.9508,  0.9254,  0.9495,  0.8811,  0.9586,  0.9656,  0.9364,\n",
      "          0.9364,  0.9880,  0.9482,  0.8734,  0.9866,  0.9896,  0.9679,  0.8680,\n",
      "          0.9103,  0.9919,  0.7652,  0.9011,  0.9504,  0.9948,  0.9635,  0.5656,\n",
      "          0.9715,  0.8534,  0.9589,  0.9350,  0.9646,  0.9928,  0.9255,  0.9779,\n",
      "          0.9775,  0.9289,  0.9709,  0.9139,  0.9427,  0.9829,  0.9050,  0.9615,\n",
      "          0.9013,  0.6925,  0.7418,  0.0492,  0.9816,  0.9977,  0.9079,  0.9897,\n",
      "          0.8405,  0.9882,  0.9741,  0.9509,  0.9607,  0.9558,  0.9872,  0.9934,\n",
      "          0.8302,  0.9107,  0.9534,  0.9977,  0.9653,  0.9155,  0.9816,  0.7807,\n",
      "          0.7784,  0.7981,  0.6468,  0.9774,  0.9868,  0.9760,  0.9636,  0.9531,\n",
      "          0.9876,  0.9653,  0.9473,  0.7400,  0.9220,  0.8724,  0.8486,  0.9794,\n",
      "          0.9899,  0.9180,  0.9616,  0.9749,  0.7450,  0.6886,  0.9758,  0.9966,\n",
      "          0.9232,  0.9797,  0.9799,  0.9370,  0.9601,  0.9976,  0.9745,  0.9727,\n",
      "          0.9770,  0.8109,  0.9458,  0.9858,  0.9506,  0.8782,  0.9618,  0.9034,\n",
      "          0.9342,  0.3132,  0.9538,  0.8981,  0.9840,  0.8981,  0.9356,  0.8998,\n",
      "          0.8632,  0.9503,  0.9445,  0.8709,  0.9451,  0.9305,  0.9923,  0.9423,\n",
      "          0.9029,  0.9267,  0.9591,  0.7344,  0.9620,  0.9643,  0.9709,  0.9654,\n",
      "          0.9698,  0.8965,  0.9177,  0.9569,  0.9155,  0.9984,  0.8753,  0.8198,\n",
      "          0.9563,  0.5540,  0.9394,  0.5194,  0.9876,  0.8810,  0.9824,  0.9592,\n",
      "          0.9577,  0.8435,  0.9235,  0.9862,  0.9882,  0.9861,  0.9722,  0.9123,\n",
      "          0.7541,  0.9397,  0.9359,  0.9560,  0.8427,  0.8998,  0.7375,  0.9337,\n",
      "          0.9671,  0.7894,  0.8656,  0.8908,  0.8513,  0.9575,  0.9849,  0.9673,\n",
      "          0.9070,  0.9221,  0.8947,  0.7014,  0.9460,  0.9475,  0.9338,  0.8990,\n",
      "          0.8785,  0.9795,  0.8834,  0.9849,  0.9602,  0.4632,  0.8878,  0.9740,\n",
      "          0.9437,  0.9750,  0.7414,  0.9175,  0.7468,  0.9682,  0.9078,  0.9888,\n",
      "          0.9474,  0.9904,  0.9338,  0.9705,  0.7504,  0.9168,  0.8383,  0.5849,\n",
      "          0.9943,  0.9641,  0.9819,  0.8859,  0.9636,  0.9578,  0.9479,  0.9408,\n",
      "          0.9541,  0.8212,  0.9489,  0.9986,  0.8926,  0.8848,  0.9409,  0.6383,\n",
      "          0.9206,  0.9808,  0.8187,  0.7881,  0.8839,  0.7953,  0.9593,  0.9481,\n",
      "          0.9640,  0.9113,  0.8858,  0.8650,  0.9904,  0.9950,  0.9939,  0.8482,\n",
      "          0.9202,  0.8086,  0.9237,  0.9834,  0.9352,  0.9507,  0.9718,  0.3536,\n",
      "          0.7809,  0.9350,  0.9668,  0.8476,  0.9604,  0.9499,  0.9207,  0.7808,\n",
      "          0.8641,  0.9563,  0.8696,  0.9409,  0.9485,  0.8703,  0.9703, -0.2432,\n",
      "          0.8068,  0.9796,  0.9338,  0.9583,  0.9915,  0.9710,  0.9294,  0.9391,\n",
      "          0.9620,  0.9271,  0.9675,  0.9637,  0.9715,  0.8791,  0.9170,  0.9435,\n",
      "          0.9867,  0.9920,  0.9230,  0.7374,  0.9594,  0.7930,  0.7768,  0.9369,\n",
      "          0.8066,  0.9774,  0.9942,  0.7670,  0.9608,  0.9852,  0.9888,  0.9603,\n",
      "          0.9256,  0.9869,  0.9133,  0.8821,  0.8793,  0.9816,  0.9821,  0.6241,\n",
      "          0.9452,  0.9848,  0.8192,  0.4072,  0.9040,  0.8951,  0.9174,  0.9702,\n",
      "          0.8967,  0.7273,  0.9612,  0.9900,  0.9867,  0.9473,  0.9584,  0.8832,\n",
      "          0.9363,  0.8471,  0.9573,  0.8770,  0.9524,  0.9262,  0.9842,  0.9389,\n",
      "          0.6651,  0.6419,  0.8999,  0.9655,  0.9257,  0.9678,  0.9704,  0.7393,\n",
      "          0.8155,  0.8097,  0.9409,  0.9795,  0.9828,  0.9818,  0.7885,  0.9766,\n",
      "          0.9686,  0.9375,  0.9730,  0.9543,  0.8970,  0.9646,  0.9266,  0.7808,\n",
      "          0.6966,  0.9797,  0.9956,  0.9234,  0.9087,  0.9434,  0.8552,  0.9761,\n",
      "          0.9049,  0.5153,  0.9586,  0.9208,  0.9918,  0.9887,  0.9237,  0.9838,\n",
      "          0.9981,  0.9604,  0.9424,  0.9375,  0.6926,  0.8205,  0.9044,  0.9675,\n",
      "          0.9745,  0.9705,  0.8815,  0.8604,  0.9868,  0.9435,  0.9843,  0.9800,\n",
      "          0.9871,  0.3179,  0.9306,  0.9934,  0.9779,  0.9385,  0.9646,  0.9850,\n",
      "          0.8552,  0.9551,  0.9738,  0.9946,  0.9703,  0.9859,  0.9376,  0.9913,\n",
      "          0.9904,  0.8629,  0.8856,  0.9666,  0.9632,  0.9548,  0.9223,  0.7897,\n",
      "          0.9396,  0.8861,  0.8132,  0.9857,  0.9259,  0.8432,  0.9826,  0.9977]],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input texts\n",
    "text1 = \"This is the first text.\"\n",
    "text2 = \"This is the second text.\"\n",
    "\n",
    "# Tokenize the input texts\n",
    "tokens1 = tokenizer.encode_plus(text1, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "tokens2 = tokenizer.encode_plus(text2, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs1 = model(**tokens1)\n",
    "outputs2 = model(**tokens2)\n",
    "\n",
    "# Get the embeddings from the model outputs\n",
    "embeddings1 = outputs1.last_hidden_state\n",
    "embeddings2 = outputs2.last_hidden_state\n",
    "\n",
    "# Compare the embeddings (e.g., using cosine similarity)\n",
    "similarity = torch.cosine_similarity(embeddings1, embeddings2)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.43587255477905273,\n",
       "  'token': 2189,\n",
       "  'token_str': 'music',\n",
       "  'sequence': 'rahul is well versed in music and dancing'},\n",
       " {'score': 0.27947288751602173,\n",
       "  'token': 4823,\n",
       "  'token_str': 'singing',\n",
       "  'sequence': 'rahul is well versed in singing and dancing'},\n",
       " {'score': 0.1585802137851715,\n",
       "  'token': 3772,\n",
       "  'token_str': 'acting',\n",
       "  'sequence': 'rahul is well versed in acting and dancing'},\n",
       " {'score': 0.0125444820150733,\n",
       "  'token': 3689,\n",
       "  'token_str': 'drama',\n",
       "  'sequence': 'rahul is well versed in drama and dancing'},\n",
       " {'score': 0.011465097777545452,\n",
       "  'token': 2299,\n",
       "  'token_str': 'song',\n",
       "  'sequence': 'rahul is well versed in song and dancing'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker(\"Rahul is well versed in [MASK] and dancing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1386,  0.1583, -0.2967,  ..., -0.2708, -0.2844,  0.4581],\n",
      "         [ 0.5364, -0.2327,  0.1754,  ...,  0.5540,  0.4981, -0.0024],\n",
      "         [ 0.3002, -0.3475,  0.1208,  ..., -0.4562,  0.3288,  0.8773],\n",
      "         ...,\n",
      "         [ 0.3799,  0.1203,  0.8283,  ..., -0.8624, -0.5957,  0.0471],\n",
      "         [-0.0252, -0.7177, -0.6950,  ...,  0.0757, -0.6668, -0.3401],\n",
      "         [ 0.7535,  0.2391,  0.0717,  ...,  0.2467, -0.6458, -0.3213]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.9377, -0.5043, -0.9799,  0.9030,  0.9329, -0.2438,  0.8926,  0.2288,\n",
      "         -0.9531, -1.0000, -0.8862,  0.9906,  0.9855,  0.7155,  0.9455, -0.8645,\n",
      "         -0.6035, -0.6666,  0.3020, -0.1587,  0.7455,  1.0000, -0.4022,  0.4261,\n",
      "          0.6151,  0.9996, -0.8773,  0.9594,  0.9585,  0.6950, -0.6718,  0.3325,\n",
      "         -0.9954, -0.2268, -0.9658, -0.9951,  0.6127, -0.7670,  0.0873,  0.0824,\n",
      "         -0.9518,  0.4713,  1.0000,  0.3299,  0.7583, -0.2670, -1.0000,  0.3166,\n",
      "         -0.9364,  0.9910,  0.9719,  0.9893,  0.2190,  0.6048,  0.5849, -0.4123,\n",
      "         -0.0063,  0.1719, -0.3988, -0.6190, -0.6603,  0.5069, -0.9757, -0.9039,\n",
      "          0.9926,  0.9323, -0.3687, -0.4869, -0.3143,  0.0499,  0.9129,  0.3396,\n",
      "         -0.1879, -0.9235,  0.8675,  0.3228, -0.6406,  1.0000, -0.7989, -0.9931,\n",
      "          0.9629,  0.9124,  0.4827, -0.7276,  0.5996, -1.0000,  0.7548, -0.1600,\n",
      "         -0.9941,  0.3386,  0.8394, -0.4158,  0.2943,  0.6111, -0.5745, -0.7185,\n",
      "         -0.4768, -0.9681, -0.4327, -0.6732,  0.1248, -0.2093, -0.5882, -0.4186,\n",
      "          0.5447, -0.6125, -0.6138,  0.4712,  0.4779,  0.7633,  0.3974, -0.4148,\n",
      "          0.7063, -0.9680,  0.7389, -0.4270, -0.9948, -0.6019, -0.9950,  0.7459,\n",
      "         -0.6343, -0.2753,  0.9522, -0.5724,  0.6218, -0.1295, -0.9905, -1.0000,\n",
      "         -0.8710, -0.7506, -0.5008, -0.4827, -0.9872, -0.9847,  0.7214,  0.9694,\n",
      "          0.3013,  1.0000, -0.4427,  0.9699, -0.5431, -0.8189,  0.9180, -0.5132,\n",
      "          0.9026,  0.5274, -0.5940,  0.2928, -0.6933,  0.7179, -0.9318, -0.2776,\n",
      "         -0.9160, -0.9457, -0.3287,  0.9556, -0.7927, -0.9860, -0.1904, -0.2760,\n",
      "         -0.6062,  0.9005,  0.9266,  0.4353, -0.6858,  0.4720,  0.2851,  0.7685,\n",
      "         -0.8647, -0.5626,  0.5127, -0.5468, -0.9490, -0.9907, -0.5809,  0.7146,\n",
      "          0.9948,  0.7981,  0.3463,  0.9349, -0.4238,  0.9333, -0.9754,  0.9936,\n",
      "         -0.2597,  0.4665, -0.5400,  0.4947, -0.8723,  0.0034,  0.8378, -0.9134,\n",
      "         -0.8432, -0.2516, -0.5177, -0.4687, -0.9491,  0.5691, -0.4856, -0.4857,\n",
      "         -0.2245,  0.9609,  0.9823,  0.7496,  0.6256,  0.8552, -0.9073, -0.5802,\n",
      "          0.2874,  0.3017,  0.3016,  0.9974, -0.8503, -0.2108, -0.9261, -0.9907,\n",
      "         -0.0252, -0.9488, -0.3972, -0.8097,  0.8707, -0.7512,  0.8107,  0.5488,\n",
      "         -0.9830, -0.8569,  0.4852, -0.6156,  0.4846, -0.2893,  0.9647,  0.9858,\n",
      "         -0.7064,  0.7120,  0.9593, -0.9590, -0.8708,  0.7893, -0.3561,  0.8603,\n",
      "         -0.7243,  0.9882,  0.9876,  0.9282, -0.9547, -0.8329, -0.7993, -0.8398,\n",
      "         -0.2333,  0.2315,  0.9712,  0.6055,  0.6388,  0.2429, -0.7884,  0.9981,\n",
      "         -0.9448, -0.9804, -0.8184, -0.3534, -0.9951,  0.9729,  0.4165,  0.8094,\n",
      "         -0.6227, -0.8183, -0.9817,  0.8532,  0.1242,  0.9826, -0.6376, -0.9450,\n",
      "         -0.8094, -0.9748,  0.0412, -0.3097, -0.8153, -0.0306, -0.9255,  0.5677,\n",
      "          0.6217,  0.6652, -0.9682,  0.9997,  1.0000,  0.9826,  0.9013,  0.8950,\n",
      "         -1.0000, -0.8081,  1.0000, -0.9995, -1.0000, -0.9361, -0.8200,  0.4755,\n",
      "         -1.0000, -0.2698, -0.0111, -0.9297,  0.8492,  0.9879,  0.9950, -1.0000,\n",
      "          0.8653,  0.9513, -0.5679,  0.9966, -0.6713,  0.9815,  0.6008,  0.7414,\n",
      "         -0.3265,  0.5574, -0.9801, -0.8956, -0.8082, -0.9267,  0.9999,  0.2542,\n",
      "         -0.7970, -0.8854,  0.7831, -0.1391, -0.0060, -0.9786, -0.4503,  0.8895,\n",
      "          0.9021,  0.3021,  0.2650, -0.5750,  0.5099,  0.1216,  0.1170,  0.6484,\n",
      "         -0.9505, -0.3889, -0.6938,  0.2508, -0.7526, -0.9831,  0.9646, -0.2742,\n",
      "          0.9865,  1.0000,  0.3756, -0.9045,  0.8847,  0.4860, -0.5515,  1.0000,\n",
      "          0.9092, -0.9904, -0.4959,  0.7900, -0.7156, -0.8280,  0.9999, -0.4197,\n",
      "         -0.9282, -0.7733,  0.9945, -0.9956,  0.9998, -0.8985, -0.9838,  0.9735,\n",
      "          0.9655, -0.8103, -0.8325,  0.1020, -0.6722,  0.4561, -0.9412,  0.8396,\n",
      "          0.6979, -0.1201,  0.9288, -0.8345, -0.6312,  0.4356, -0.8901, -0.4565,\n",
      "          0.9874,  0.5709, -0.2111, -0.0206, -0.4182, -0.9116, -0.9781,  0.8246,\n",
      "          1.0000, -0.4229,  0.9489, -0.5226, -0.0986,  0.2202,  0.7459,  0.7152,\n",
      "         -0.3528, -0.8800,  0.9299, -0.9716, -0.9949,  0.7278,  0.2206, -0.4944,\n",
      "          1.0000,  0.6285,  0.3795,  0.7228,  0.9993,  0.0301,  0.5936,  0.9816,\n",
      "          0.9914, -0.3465,  0.5882,  0.8365, -0.9824, -0.4488, -0.7612,  0.1331,\n",
      "         -0.9479, -0.0559, -0.9697,  0.9846,  0.9960,  0.5818,  0.3121,  0.8577,\n",
      "          1.0000, -0.9274,  0.6693, -0.1365,  0.8035, -1.0000, -0.8057, -0.4504,\n",
      "         -0.1711, -0.9512, -0.5899,  0.3991, -0.9754,  0.9563,  0.8806, -0.9937,\n",
      "         -0.9923, -0.4979,  0.8853,  0.1439, -0.9994, -0.8986, -0.6272,  0.8385,\n",
      "         -0.3239, -0.9470, -0.7009, -0.4768,  0.5742, -0.2216,  0.5665,  0.9667,\n",
      "          0.7935, -0.9401, -0.6746, -0.1753, -0.9163,  0.9409, -0.8701, -0.9894,\n",
      "         -0.2514,  1.0000, -0.4087,  0.9385,  0.6050,  0.8219, -0.2712,  0.3326,\n",
      "          0.9827,  0.3613, -0.8314, -0.9850, -0.2861, -0.5398,  0.8254,  0.8414,\n",
      "          0.7590,  0.9412,  0.9627,  0.2765, -0.0737,  0.0399,  0.9998, -0.3095,\n",
      "         -0.1933, -0.4689, -0.2511, -0.4629, -0.2914,  1.0000,  0.3963,  0.7777,\n",
      "         -0.9950, -0.9808, -0.9303,  1.0000,  0.8822, -0.6848,  0.8124,  0.6242,\n",
      "         -0.2551,  0.8266, -0.2791, -0.3167,  0.2294,  0.1682,  0.9627, -0.6738,\n",
      "         -0.9904, -0.7910,  0.7099, -0.9770,  1.0000, -0.7030, -0.3960, -0.5981,\n",
      "         -0.6683, -0.2727, -0.0183, -0.9882, -0.3841,  0.5605,  0.9745,  0.3505,\n",
      "         -0.4898, -0.9298,  0.9578,  0.9533, -0.9859, -0.9597,  0.9777, -0.9784,\n",
      "          0.7550,  1.0000,  0.3446,  0.6786,  0.3947, -0.5349,  0.5541, -0.6754,\n",
      "          0.8078, -0.9595, -0.4484, -0.3901,  0.3983, -0.1319, -0.2896,  0.7860,\n",
      "          0.3500, -0.5530, -0.7294, -0.2361,  0.4663,  0.9332, -0.3048, -0.1916,\n",
      "          0.2318, -0.3230, -0.9323, -0.4672, -0.6315, -1.0000,  0.8068, -1.0000,\n",
      "          0.8035,  0.4066, -0.3700,  0.8760,  0.7829,  0.8298, -0.8628, -0.9795,\n",
      "          0.1322,  0.8529, -0.5029, -0.9057, -0.6918,  0.5017, -0.2052,  0.1564,\n",
      "         -0.7397,  0.8156, -0.3414,  1.0000,  0.2659, -0.8292, -0.9821,  0.2491,\n",
      "         -0.3009,  1.0000, -0.8952, -0.9832,  0.3330, -0.9180, -0.8493,  0.5868,\n",
      "          0.1653, -0.8522, -0.9961,  0.9220,  0.8661, -0.6477,  0.7927, -0.3991,\n",
      "         -0.7691,  0.1512,  0.9868,  0.9924,  0.7317,  0.9083, -0.1227, -0.5258,\n",
      "          0.9840,  0.4009, -0.0436,  0.1361,  1.0000,  0.4004, -0.9497, -0.1309,\n",
      "         -0.9788, -0.3522, -0.9551,  0.3755,  0.3099,  0.9195, -0.4460,  0.9738,\n",
      "         -0.9714,  0.1901, -0.8894, -0.7863,  0.4757, -0.9463, -0.9892, -0.9938,\n",
      "          0.8142, -0.4077, -0.1895,  0.2102,  0.1715,  0.6322,  0.5566, -1.0000,\n",
      "          0.9642,  0.6150,  0.9768,  0.9768,  0.9115,  0.8108,  0.3251, -0.9920,\n",
      "         -0.9910, -0.5438, -0.3567,  0.7960,  0.7648,  0.8900,  0.6470, -0.4875,\n",
      "         -0.4792, -0.7756, -0.8423, -0.9972,  0.5961, -0.8679, -0.9678,  0.9718,\n",
      "         -0.3461, -0.1534, -0.2139, -0.9586,  0.9321,  0.7627,  0.4636,  0.0862,\n",
      "          0.5071,  0.9170,  0.9597,  0.9882, -0.9231,  0.8555, -0.9196,  0.6712,\n",
      "          0.9381, -0.9606,  0.2335,  0.8301, -0.5560,  0.3696, -0.4752, -0.9740,\n",
      "          0.8174, -0.4268,  0.7773, -0.4798,  0.0639, -0.4718, -0.2607, -0.7624,\n",
      "         -0.8742,  0.6576,  0.6207,  0.9219,  0.9360, -0.0496, -0.8942, -0.3701,\n",
      "         -0.8944, -0.9526,  0.9536, -0.0851, -0.2961,  0.9031,  0.1321,  0.9324,\n",
      "          0.4289, -0.4989, -0.4174, -0.7639,  0.8887, -0.7894, -0.7639, -0.7093,\n",
      "          0.8105,  0.3595,  1.0000, -0.9188, -0.9878, -0.8268, -0.6012,  0.4992,\n",
      "         -0.7880, -1.0000,  0.3609, -0.8314,  0.8524, -0.9398,  0.9500, -0.9339,\n",
      "         -0.9851, -0.3495,  0.8436,  0.9375, -0.5159, -0.8989,  0.5196, -0.8797,\n",
      "          0.9979,  0.8753, -0.8277, -0.0012,  0.6013, -0.9184, -0.7398,  0.9228]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\test.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m embeddings \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# Average pooling of token embeddings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W4sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# # Calculate the cosine similarity between the sentence embeddings\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W4sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m similarity \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m cosine(embeddings[\u001b[39m0\u001b[39;49m], embeddings[\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\distance.py:668\u001b[0m, in \u001b[0;36mcosine\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[39mCompute the Cosine distance between 1-D arrays.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    663\u001b[0m \n\u001b[0;32m    664\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[39m# cosine distance is also referred to as 'uncentered correlation',\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[39m#   or 'reflective correlation'\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[39m# clamp the result to 0-2\u001b[39;00m\n\u001b[1;32m--> 668\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mmin\u001b[39m(correlation(u, v, w\u001b[39m=\u001b[39;49mw, centered\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m), \u001b[39m2.0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\distance.py:608\u001b[0m, in \u001b[0;36mcorrelation\u001b[1;34m(u, v, w, centered)\u001b[0m\n\u001b[0;32m    575\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrelation\u001b[39m(u, v, w\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, centered\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    576\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[39m    Compute the correlation distance between two 1-D arrays.\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    606\u001b[0m \n\u001b[0;32m    607\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 608\u001b[0m     u \u001b[39m=\u001b[39m _validate_vector(u)\n\u001b[0;32m    609\u001b[0m     v \u001b[39m=\u001b[39m _validate_vector(v)\n\u001b[0;32m    610\u001b[0m     \u001b[39mif\u001b[39;00m w \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\distance.py:298\u001b[0m, in \u001b[0;36m_validate_vector\u001b[1;34m(u, dtype)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_vector\u001b[39m(u, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    297\u001b[0m     \u001b[39m# XXX Is order='c' really necessary?\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     u \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(u, dtype\u001b[39m=\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m u\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    300\u001b[0m         \u001b[39mreturn\u001b[39;00m u\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example sentences\n",
    "sentence1 = \"The cat sits on the mat.\"\n",
    "sentence2 = \"The dog plays in the park.\"\n",
    "\n",
    "# Tokenize and encode the sentences\n",
    "tokens = tokenizer([sentence1, sentence2], padding=True, truncation=True, return_tensors='pt')\n",
    "outputs = model(**tokens)\n",
    "\n",
    "# # Get the sentence embeddings\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling of token embeddings\n",
    "\n",
    "# # Calculate the cosine similarity between the sentence embeddings\n",
    "similarity = 1 - cosine(embeddings[0], embeddings[1])\n",
    "\n",
    "# print(f\"Similarity: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGetoptError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\_distutils\\fancy_getopt.py:246\u001b[0m, in \u001b[0;36mFancyGetopt.getopt\u001b[1;34m(self, args, object)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 246\u001b[0m     opts, args \u001b[39m=\u001b[39m getopt\u001b[39m.\u001b[39;49mgetopt(args, short_opts, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlong_opts)\n\u001b[0;32m    247\u001b[0m \u001b[39mexcept\u001b[39;00m getopt\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m msg:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\getopt.py:93\u001b[0m, in \u001b[0;36mgetopt\u001b[1;34m(args, shortopts, longopts)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m     opts, args \u001b[39m=\u001b[39m do_longs(opts, args[\u001b[39m0\u001b[39;49m][\u001b[39m2\u001b[39;49m:], longopts, args[\u001b[39m1\u001b[39;49m:])\n\u001b[0;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\getopt.py:157\u001b[0m, in \u001b[0;36mdo_longs\u001b[1;34m(opts, opt, longopts, args)\u001b[0m\n\u001b[0;32m    155\u001b[0m     opt, optarg \u001b[39m=\u001b[39m opt[:i], opt[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[1;32m--> 157\u001b[0m has_arg, opt \u001b[39m=\u001b[39m long_has_args(opt, longopts)\n\u001b[0;32m    158\u001b[0m \u001b[39mif\u001b[39;00m has_arg:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\getopt.py:174\u001b[0m, in \u001b[0;36mlong_has_args\u001b[1;34m(opt, longopts)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m possibilities:\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mraise\u001b[39;00m GetoptError(_(\u001b[39m'\u001b[39m\u001b[39moption --\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m not recognized\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m%\u001b[39m opt, opt)\n\u001b[0;32m    175\u001b[0m \u001b[39m# Is there an exact match?\u001b[39;00m\n",
      "\u001b[1;31mGetoptError\u001b[0m: option --ip not recognized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDistutilsArgError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\_distutils\\core.py:172\u001b[0m, in \u001b[0;36msetup\u001b[1;34m(**attrs)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     ok \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39;49mparse_command_line()\n\u001b[0;32m    173\u001b[0m \u001b[39mexcept\u001b[39;00m DistutilsArgError \u001b[39mas\u001b[39;00m msg:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\_distutils\\dist.py:467\u001b[0m, in \u001b[0;36mDistribution.parse_command_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    466\u001b[0m parser\u001b[39m.\u001b[39mset_aliases({\u001b[39m'\u001b[39m\u001b[39mlicence\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mlicense\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m--> 467\u001b[0m args \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mgetopt(args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscript_args, \u001b[39mobject\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    468\u001b[0m option_order \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39mget_option_order()\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\_distutils\\fancy_getopt.py:248\u001b[0m, in \u001b[0;36mFancyGetopt.getopt\u001b[1;34m(self, args, object)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mexcept\u001b[39;00m getopt\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m msg:\n\u001b[1;32m--> 248\u001b[0m     \u001b[39mraise\u001b[39;00m DistutilsArgError(msg)\n\u001b[0;32m    250\u001b[0m \u001b[39mfor\u001b[39;00m opt, val \u001b[39min\u001b[39;00m opts:\n",
      "\u001b[1;31mDistutilsArgError\u001b[0m: option --ip not recognized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\test.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m long_description \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mEasily train your own text-generating neural network of\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39many size and complexity on any text dataset with a few lines\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m- Able to generate text interactively for customized stories.\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m'''\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m setup(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtextgenrnn\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     packages\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mtextgenrnn\u001b[39;49m\u001b[39m'\u001b[39;49m],  \u001b[39m# this must be the same as the name above\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     version\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m2.0.0\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     description\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mEasily train your own text-generating neural network \u001b[39;49m\u001b[39m'\u001b[39;49m \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mof any size and complexity\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     long_description\u001b[39m=\u001b[39;49mlong_description,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     long_description_content_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtext/markdown\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     author\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMax Woolf\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     author_email\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmax@minimaxir.com\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     url\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhttps://github.com/minimaxir/textgenrnn\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     keywords\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mdeep learning\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtensorflow\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mkeras\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtext generation\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     classifiers\u001b[39m=\u001b[39;49m[],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     license\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMIT\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     python_requires\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m>=3.5\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     include_package_data\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     install_requires\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mh5py\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mscikit-learn\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtqdm\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtensorflow>=2.1.0\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\__init__.py:107\u001b[0m, in \u001b[0;36msetup\u001b[1;34m(**attrs)\u001b[0m\n\u001b[0;32m    106\u001b[0m _install_setup_requires(attrs)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m distutils\u001b[39m.\u001b[39mcore\u001b[39m.\u001b[39msetup(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattrs)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\setuptools\\_distutils\\core.py:174\u001b[0m, in \u001b[0;36msetup\u001b[1;34m(**attrs)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mexcept\u001b[39;00m DistutilsArgError \u001b[39mas\u001b[39;00m msg:\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mSystemExit\u001b[39;00m(gen_usage(dist\u001b[39m.\u001b[39mscript_name) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39merror: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m msg)\n\u001b[0;32m    176\u001b[0m \u001b[39mif\u001b[39;00m DEBUG:\n",
      "\u001b[1;31mSystemExit\u001b[0m: usage: ipykernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: ipykernel_launcher.py --help [cmd1 cmd2 ...]\n   or: ipykernel_launcher.py --help-commands\n   or: ipykernel_launcher.py cmd --help\n\nerror: option --ip not recognized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:1983\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   1980\u001b[0m \u001b[39mif\u001b[39;00m exception_only:\n\u001b[0;32m   1981\u001b[0m     stb \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAn exception has occurred, use \u001b[39m\u001b[39m%\u001b[39m\u001b[39mtb to see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1982\u001b[0m            \u001b[39m'\u001b[39m\u001b[39mthe full traceback.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m]\n\u001b[1;32m-> 1983\u001b[0m     stb\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mInteractiveTB\u001b[39m.\u001b[39;49mget_exception_only(etype,\n\u001b[0;32m   1984\u001b[0m                                                      value))\n\u001b[0;32m   1985\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1986\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1987\u001b[0m         \u001b[39m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[0;32m   1988\u001b[0m         \u001b[39m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[0;32m   1989\u001b[0m         \u001b[39m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py:585\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_exception_only\u001b[39m(\u001b[39mself\u001b[39m, etype, value):\n\u001b[0;32m    578\u001b[0m     \u001b[39m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \n\u001b[0;32m    580\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[39m    value : exception value\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39;49mstructured_traceback(\u001b[39mself\u001b[39;49m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py:443\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    440\u001b[0m     chained_exc_ids\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(exception[\u001b[39m1\u001b[39m]))\n\u001b[0;32m    441\u001b[0m     chained_exceptions_tb_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    442\u001b[0m     out_list \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 443\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m    444\u001b[0m             etype, evalue, (etb, chained_exc_ids),\n\u001b[0;32m    445\u001b[0m             chained_exceptions_tb_offset, context)\n\u001b[0;32m    446\u001b[0m         \u001b[39m+\u001b[39m chained_exception_message\n\u001b[0;32m    447\u001b[0m         \u001b[39m+\u001b[39m out_list)\n\u001b[0;32m    449\u001b[0m \u001b[39mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py:1118\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1117\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtb \u001b[39m=\u001b[39m tb\n\u001b[1;32m-> 1118\u001b[0m \u001b[39mreturn\u001b[39;00m FormattedTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m   1119\u001b[0m     \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py:1012\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1009\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode\n\u001b[0;32m   1010\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose_modes:\n\u001b[0;32m   1011\u001b[0m     \u001b[39m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1012\u001b[0m     \u001b[39mreturn\u001b[39;00m VerboseTB\u001b[39m.\u001b[39;49mstructured_traceback(\n\u001b[0;32m   1013\u001b[0m         \u001b[39mself\u001b[39;49m, etype, value, tb, tb_offset, number_of_lines_of_context\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1015\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mMinimal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m   1016\u001b[0m     \u001b[39mreturn\u001b[39;00m ListTB\u001b[39m.\u001b[39mget_exception_only(\u001b[39mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py:865\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstructured_traceback\u001b[39m(\n\u001b[0;32m    857\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    858\u001b[0m     etype: \u001b[39mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    862\u001b[0m     number_of_lines_of_context: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[0;32m    863\u001b[0m ):\n\u001b[0;32m    864\u001b[0m     \u001b[39m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 865\u001b[0m     formatted_exception \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m    866\u001b[0m                                                            tb_offset)\n\u001b[0;32m    868\u001b[0m     colors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mColors  \u001b[39m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m    869\u001b[0m     colorsnormal \u001b[39m=\u001b[39m colors\u001b[39m.\u001b[39mNormal  \u001b[39m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py:799\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tb_offset, \u001b[39mint\u001b[39m)\n\u001b[0;32m    797\u001b[0m head \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_header(etype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlong_header)\n\u001b[0;32m    798\u001b[0m records \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 799\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_records(etb, number_of_lines_of_context, tb_offset) \u001b[39mif\u001b[39;00m etb \u001b[39melse\u001b[39;00m []\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m frames \u001b[39m=\u001b[39m []\n\u001b[0;32m    803\u001b[0m skipped \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py:854\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    848\u001b[0m     formatter \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    849\u001b[0m options \u001b[39m=\u001b[39m stack_data\u001b[39m.\u001b[39mOptions(\n\u001b[0;32m    850\u001b[0m     before\u001b[39m=\u001b[39mbefore,\n\u001b[0;32m    851\u001b[0m     after\u001b[39m=\u001b[39mafter,\n\u001b[0;32m    852\u001b[0m     pygments_formatter\u001b[39m=\u001b[39mformatter,\n\u001b[0;32m    853\u001b[0m )\n\u001b[1;32m--> 854\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(stack_data\u001b[39m.\u001b[39;49mFrameInfo\u001b[39m.\u001b[39;49mstack_data(etb, options\u001b[39m=\u001b[39;49moptions))[tb_offset:]\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py:546\u001b[0m, in \u001b[0;36mFrameInfo.stack_data\u001b[1;34m(cls, frame_or_tb, options, collapse_repeated_frames)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    531\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstack_data\u001b[39m(\n\u001b[0;32m    532\u001b[0m         \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m         collapse_repeated_frames: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    537\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Union[\u001b[39m'\u001b[39m\u001b[39mFrameInfo\u001b[39m\u001b[39m'\u001b[39m, RepeatedFrames]]:\n\u001b[0;32m    538\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[39m    An iterator of FrameInfo and RepeatedFrames objects representing\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39m    a full traceback or stack. Similar consecutive frames are collapsed into RepeatedFrames\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[39m    and optionally an Options object to configure.\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 546\u001b[0m     stack \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(iter_stack(frame_or_tb))\n\u001b[0;32m    548\u001b[0m     \u001b[39m# Reverse the stack from a frame so that it's in the same order\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[39m# as the order from a traceback, which is the order of a printed\u001b[39;00m\n\u001b[0;32m    550\u001b[0m     \u001b[39m# traceback when read top to bottom (most recent call last)\u001b[39;00m\n\u001b[0;32m    551\u001b[0m     \u001b[39mif\u001b[39;00m is_frame(frame_or_tb):\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\utils.py:98\u001b[0m, in \u001b[0;36miter_stack\u001b[1;34m(frame_or_tb)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mwhile\u001b[39;00m frame_or_tb:\n\u001b[0;32m     97\u001b[0m     \u001b[39myield\u001b[39;00m frame_or_tb\n\u001b[1;32m---> 98\u001b[0m     \u001b[39mif\u001b[39;00m is_frame(frame_or_tb):\n\u001b[0;32m     99\u001b[0m         frame_or_tb \u001b[39m=\u001b[39m frame_or_tb\u001b[39m.\u001b[39mf_back\n\u001b[0;32m    100\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\utils.py:91\u001b[0m, in \u001b[0;36mis_frame\u001b[1;34m(frame_or_tb)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_frame\u001b[39m(frame_or_tb: Union[FrameType, TracebackType]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m---> 91\u001b[0m     assert_(\u001b[39misinstance\u001b[39;49m(frame_or_tb, (types\u001b[39m.\u001b[39;49mFrameType, types\u001b[39m.\u001b[39;49mTracebackType)))\n\u001b[0;32m     92\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(frame_or_tb, (types\u001b[39m.\u001b[39mFrameType,))\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\utils.py:172\u001b[0m, in \u001b[0;36massert_\u001b[1;34m(condition, error)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(error, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    171\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39mAssertionError\u001b[39;00m(error)\n\u001b[1;32m--> 172\u001b[0m \u001b[39mraise\u001b[39;00m error\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from setuptools import setup, find_packages\n",
    "\n",
    "long_description = '''\n",
    "Easily train your own text-generating neural network of\n",
    "any size and complexity on any text dataset with a few lines\n",
    "of code, or quickly train on a text using a pretrained model.\n",
    "\n",
    "- A modern neural network architecture which utilizes new techniques as\n",
    "attention-weighting and skip-embedding to accelerate training\n",
    "and improve model quality.\n",
    "- Able to train on and generate text at either the\n",
    "character-level or word-level.\n",
    "- Able to configure RNN size, the number of RNN layers,\n",
    "and whether to use bidirectional RNNs.\n",
    "- Able to train on any generic input text file, including large files.\n",
    "- Able to train models on a GPU and then use them with a CPU.\n",
    "- Able to utilize a powerful CuDNN implementation of RNNs\n",
    "when trained on the GPU, which massively speeds up training time as\n",
    "opposed to normal LSTM implementations.\n",
    "- Able to train the model using contextual labels,\n",
    "allowing it to learn faster and produce better results in some cases.\n",
    "- Able to generate text interactively for customized stories.\n",
    "'''\n",
    "\n",
    "\n",
    "setup(\n",
    "    name='textgenrnn',\n",
    "    packages=['textgenrnn'],  # this must be the same as the name above\n",
    "    version='2.0.0',\n",
    "    description='Easily train your own text-generating neural network ' \\\n",
    "    'of any size and complexity',\n",
    "    long_description=long_description,\n",
    "    long_description_content_type='text/markdown',\n",
    "    author='Max Woolf',\n",
    "    author_email='max@minimaxir.com',\n",
    "    url='https://github.com/minimaxir/textgenrnn',\n",
    "    keywords=['deep learning', 'tensorflow', 'keras', 'text generation'],\n",
    "    classifiers=[],\n",
    "    license='MIT',\n",
    "    python_requires='>=3.5',\n",
    "    include_package_data=True,\n",
    "    install_requires=['h5py', 'scikit-learn', 'tqdm', 'tensorflow>=2.1.0']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "locate_python = sys.exec_prefix\n",
    "\n",
    "print(locate_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Transcribing texts #\n",
      "\n",
      "cuda\n",
      " Artificial intelligence is a branch of computer science and engineering that focuses on developing intelligent machines capable of performing quantitative tasks that are traditionally associated with human beings. Data structure is a collection of data elements that are organized in a particular way to facilitate efficient processing of data, different types of data structure or stutter to different kinds of applications depending on the nature of data and the operation that need to be performed. Data planning is a type of artificial intelligence that relies on artificial neural network with multiple layers to process and analyze large amount of data. These networks are trained using algorithms that adjust the weights and biases of the connections between neurons to optimize the performance on a specific task. Machine learning is a process of teaching computer to learn from the data without being explicitly programmed by analyzing and finding patterns in a large dataset. Machine learning algorithm can be used to make predictions or decisions in a variety of applications. Because its core data science is about using tools and techniques to identify patterns, generate insights and make prediction based on data, this involves working with large and complex dataset and using statistical methods and machine learning algorithm to analyze the data and uncover hidden patterns.\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "import torch\n",
    "import whisper\n",
    "from typing import NewType\n",
    "import warnings\n",
    "import timeit\n",
    "from functools import lru_cache\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# path_of_audio = NewType('path_of_i_th_audio',str)\n",
    "\n",
    "def banner(text):\n",
    "    # \"\"\"Display a message when the script is working in the background\"\"\"\n",
    "    print(f\"# {text} #\\n\")\n",
    "\n",
    "\n",
    "def check_device():\n",
    "    \n",
    "    # \"\"\"Check CUDA availability.\"\"\"\n",
    "    if torch.cuda.is_available() == 1:\n",
    "        device = \"cuda\"\n",
    "        \n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(device)\n",
    "    return device\n",
    "\n",
    "# \"\"\"Get speech recognition model.\"\"\"\n",
    "# model_name = input(\"Select speech recognition model name (tiny, base, small, medium, large): \")\n",
    "@lru_cache(maxsize=None)  # Decorator to enable caching\n",
    "def load_model(model_name):\n",
    "    \n",
    "    return whisper.load_model(model_name, device=check_device())\n",
    "\n",
    "def convertText(AUDIOFILE):\n",
    "    \n",
    "    # warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    #choose a mode defaulted\n",
    "    \"\"\"tiny\"\"\"\n",
    "\n",
    "    model_name = \"base\"\n",
    "\n",
    "    banner(\"Transcribing texts\")\n",
    "    model = load_model(model_name)\n",
    "        \n",
    "    # for i in range(0,len(AUDIOFILE)):\n",
    "        \n",
    "    result = model.transcribe(AUDIOFILE)\n",
    "        \n",
    "    # print(\"Result: \",result[\"text\"])\n",
    "    \n",
    "        \n",
    "    # warnings.resetwarnings()\n",
    "    return result[\"text\"]\n",
    "    \n",
    "    # return list_temp\n",
    "    \n",
    "# measure execution time of my_function\n",
    "print(convertText(r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\out1.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "import torch\n",
    "import whisper\n",
    "\n",
    "class SpeechRecognitionModel:\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if not cls._instance:\n",
    "            cls._instance = super(SpeechRecognitionModel, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        if not hasattr(self, \"model\"):\n",
    "            self.model = whisper.load_model(model_name, device=self.check_device())\n",
    "\n",
    "    def check_device(self):\n",
    "        if torch.cuda.is_available() == 1:\n",
    "            return \"cuda\"\n",
    "        else:\n",
    "            return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcriber.py\n",
    "\n",
    "# from model import SpeechRecognitionModel\n",
    "\n",
    "def convertText(AUDIOFILE):\n",
    "    list_temp = []\n",
    "    model_name = \"base\"\n",
    "    model = SpeechRecognitionModel(model_name)\n",
    "    banner(\"Transcribing texts\")\n",
    "    result = model.model.transcribe(AUDIOFILE)\n",
    "    list_temp.append(result[\"text\"])\n",
    "    return list_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Transcribing texts #\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Artificial intelligence is a branch of computer science and engineering that focuses on developing intelligent machines capable of performing quantitative tasks that are traditionally associated with human beings. Data structure is a collection of data elements that are organized in a particular way to facilitate efficient processing of data, different types of data structure or stutter to different kinds of applications depending on the nature of data and the operation that need to be performed. Data planning is a type of artificial intelligence that relies on artificial neural network with multiple layers to process and analyze large amount of data. These networks are trained using algorithms that adjust the weights and biases of the connections between neurons to optimize the performance on a specific task. Machine learning is a process of teaching computer to learn from the data without being explicitly programmed by analyzing and finding patterns in a large dataset. Machine learning algorithm can be used to make predictions or decisions in a variety of applications. Because its core data science is about using tools and techniques to identify patterns, generate insights and make prediction based on data, this involves working with large and complex dataset and using statistical methods and machine learning algorithm to analyze the data and uncover hidden patterns.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertText(r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\out1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Transcribing texts #\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Artificial intelligence is a branch of computer science and engineering that focuses on developing intelligent machines capable of performing quantitative tasks that are traditionally associated with human beings. Data structure is a collection of data elements that are organized in a particular way to facilitate efficient processing of data, different types of data structure or stutter to different kinds of applications depending on the nature of data and the operation that need to be performed. Data planning is a type of artificial intelligence that relies on artificial neural network with multiple layers to process and analyze large amount of data. These networks are trained using algorithms that adjust the weights and biases of the connections between neurons to optimize the performance on a specific task. Machine learning is a process of teaching computer to learn from the data without being explicitly programmed by analyzing and finding patterns in a large dataset. Machine learning algorithm can be used to make predictions or decisions in a variety of applications. Because its core data science is about using tools and techniques to identify patterns, generate insights and make prediction based on data, this involves working with large and complex dataset and using statistical methods and machine learning algorithm to analyze the data and uncover hidden patterns.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convertText(r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\out1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='D:\\\\Projects and codes\\\\interview\\\\resources\\\\extinsion_interview\\\\out_trimed.wav'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def remove_silence(audio_file, silence_threshold=30):\n",
    "    audio = AudioSegment.from_file(audio_file)\n",
    "\n",
    "    # Convert audio to numpy array\n",
    "    audio_data = np.array(audio.get_array_of_samples())\n",
    "\n",
    "    # Calculate the amplitude of the audio\n",
    "    audio_amplitude = np.abs(audio_data)\n",
    "\n",
    "    # Calculate the threshold in dBFS (decibels relative to full scale)\n",
    "    threshold = audio.rms + silence_threshold\n",
    "\n",
    "    # Find the indices where the audio is above the threshold\n",
    "    non_silent_indices = np.where(audio_amplitude > threshold)[0]\n",
    "\n",
    "    # Find the start and end indices of the non-silent regions\n",
    "    start_index = non_silent_indices[0]\n",
    "    end_index = non_silent_indices[-1]\n",
    "\n",
    "    # Trim the audio based on the start and end indices\n",
    "    trimmed_audio = audio[start_index:end_index]\n",
    "\n",
    "    return trimmed_audio\n",
    "\n",
    "# Usage\n",
    "audio_file = r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\out.mp3\"\n",
    "trimmed_audio = remove_silence(audio_file)\n",
    "trimmed_audio.export(r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\out_trimed.wav\", format=\"WAV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_leading_silence, detect_leading_silence\n",
    "\n",
    "def trim_silence(input_file, output_file, silence_threshold=-25.0, silence_duration=500):\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "\n",
    "    # Trim silence from the beginning\n",
    "    start_trim = detect_leading_silence(audio, silence_threshold=silence_threshold)\n",
    "    audio_trimmed = audio[start_trim:]\n",
    "\n",
    "    # Trim silence from the end\n",
    "    end_trim = detect_leading_silence(audio_trimmed.reverse(), silence_threshold=silence_threshold)\n",
    "    audio_trimmed = audio_trimmed[:len(audio_trimmed)-end_trim]\n",
    "\n",
    "    audio_trimmed.export(output_file, format='mp3')\n",
    "\n",
    "# Example usage\n",
    "input_file = r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\out.mp3\"\n",
    "output_file = r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\out_trimmed.mp3\"\n",
    "trim_silence(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    # Calculate cosine similarity between two embeddings\n",
    "    similarity_score = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\test.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(embeddings)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m getScore(\u001b[39m\"\u001b[39;49m\u001b[39mRahul is a vertain in computer science\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mRahul hates summer season and rainy seasons\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\test.ipynb Cell 17\u001b[0m in \u001b[0;36mgetScore\u001b[1;34m(sentence1, sentence2)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetScore\u001b[39m(sentence1, sentence2):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m# Your preprocessing code here\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Create embeddings\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m[sentence1, sentence2],\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-similarity-davinci-001\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Process the response\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m response[\u001b[39m'\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\embedding.py:33\u001b[0m, in \u001b[0;36mEmbedding.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     35\u001b[0m         \u001b[39m# If a user specifies base64, we'll just return the encoded string.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[39m# This is only for the default case.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_provided_encoding_format:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:230\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    211\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    219\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    220\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    221\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    222\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    228\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    231\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:624\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    617\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    618\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    619\u001b[0m         )\n\u001b[0;32m    620\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    621\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 624\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    625\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    626\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    627\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    628\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    629\u001b[0m         ),\n\u001b[0;32m    630\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    631\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\api_requestor.py:687\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    685\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    686\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    688\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    689\u001b[0m     )\n\u001b[0;32m    690\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set up OpenAI API credentials\n",
    "openai.api_key = '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'\n",
    "\n",
    "def getScore(sentence1, sentence2):\n",
    "    # Your preprocessing code here\n",
    "    \n",
    "    # Create embeddings\n",
    "    response = openai.Embedding.create(\n",
    "        input=[sentence1, sentence2],\n",
    "        engine=\"text-similarity-davinci-001\"\n",
    "    )\n",
    "    \n",
    "    # Process the response\n",
    "    embeddings = response['embeddings']\n",
    "    print(embeddings)\n",
    "\n",
    "getScore(\"Rahul is a vertain in computer science\",\"Rahul hates summer season and rainy seasons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\test.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Usage\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m audio_url \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mProjects and codes\u001b[39m\u001b[39m\\\u001b[39m\u001b[39minterview\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mresources\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mextinsion_interview\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mout.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m transcript \u001b[39m=\u001b[39m transcribe_audio(audio_url)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X23sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTranscript:\u001b[39m\u001b[39m\"\u001b[39m, transcript)\n",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\test.ipynb Cell 18\u001b[0m in \u001b[0;36mtranscribe_audio\u001b[1;34m(audio_path)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtranscribe_audio\u001b[39m(audio_path):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     client \u001b[39m=\u001b[39m speech\u001b[39m.\u001b[39;49mSpeechClient()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Read the audio file\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(audio_path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m audio_file:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\speech_v1\\services\\speech\\client.py:459\u001b[0m, in \u001b[0;36mSpeechClient.__init__\u001b[1;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[0;32m    454\u001b[0m     credentials \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39mauth\u001b[39m.\u001b[39m_default\u001b[39m.\u001b[39mget_api_key_credentials(\n\u001b[0;32m    455\u001b[0m         api_key_value\n\u001b[0;32m    456\u001b[0m     )\n\u001b[0;32m    458\u001b[0m Transport \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mget_transport_class(transport)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transport \u001b[39m=\u001b[39m Transport(\n\u001b[0;32m    460\u001b[0m     credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[0;32m    461\u001b[0m     credentials_file\u001b[39m=\u001b[39;49mclient_options\u001b[39m.\u001b[39;49mcredentials_file,\n\u001b[0;32m    462\u001b[0m     host\u001b[39m=\u001b[39;49mapi_endpoint,\n\u001b[0;32m    463\u001b[0m     scopes\u001b[39m=\u001b[39;49mclient_options\u001b[39m.\u001b[39;49mscopes,\n\u001b[0;32m    464\u001b[0m     client_cert_source_for_mtls\u001b[39m=\u001b[39;49mclient_cert_source_func,\n\u001b[0;32m    465\u001b[0m     quota_project_id\u001b[39m=\u001b[39;49mclient_options\u001b[39m.\u001b[39;49mquota_project_id,\n\u001b[0;32m    466\u001b[0m     client_info\u001b[39m=\u001b[39;49mclient_info,\n\u001b[0;32m    467\u001b[0m     always_use_jwt_access\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    468\u001b[0m     api_audience\u001b[39m=\u001b[39;49mclient_options\u001b[39m.\u001b[39;49mapi_audience,\n\u001b[0;32m    469\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\speech_v1\\services\\speech\\transports\\grpc.py:153\u001b[0m, in \u001b[0;36mSpeechGrpcTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ssl_channel_credentials \u001b[39m=\u001b[39m grpc\u001b[39m.\u001b[39mssl_channel_credentials(\n\u001b[0;32m    149\u001b[0m                 certificate_chain\u001b[39m=\u001b[39mcert, private_key\u001b[39m=\u001b[39mkey\n\u001b[0;32m    150\u001b[0m             )\n\u001b[0;32m    152\u001b[0m \u001b[39m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    154\u001b[0m     host\u001b[39m=\u001b[39;49mhost,\n\u001b[0;32m    155\u001b[0m     credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[0;32m    156\u001b[0m     credentials_file\u001b[39m=\u001b[39;49mcredentials_file,\n\u001b[0;32m    157\u001b[0m     scopes\u001b[39m=\u001b[39;49mscopes,\n\u001b[0;32m    158\u001b[0m     quota_project_id\u001b[39m=\u001b[39;49mquota_project_id,\n\u001b[0;32m    159\u001b[0m     client_info\u001b[39m=\u001b[39;49mclient_info,\n\u001b[0;32m    160\u001b[0m     always_use_jwt_access\u001b[39m=\u001b[39;49malways_use_jwt_access,\n\u001b[0;32m    161\u001b[0m     api_audience\u001b[39m=\u001b[39;49mapi_audience,\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    164\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grpc_channel:\n\u001b[0;32m    165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grpc_channel \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreate_channel(\n\u001b[0;32m    166\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_host,\n\u001b[0;32m    167\u001b[0m         \u001b[39m# use the credentials which are saved\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    178\u001b[0m         ],\n\u001b[0;32m    179\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\cloud\\speech_v1\\services\\speech\\transports\\base.py:101\u001b[0m, in \u001b[0;36mSpeechTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[0;32m     97\u001b[0m     credentials, _ \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39mauth\u001b[39m.\u001b[39mload_credentials_from_file(\n\u001b[0;32m     98\u001b[0m         credentials_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mscopes_kwargs, quota_project_id\u001b[39m=\u001b[39mquota_project_id\n\u001b[0;32m     99\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[39melif\u001b[39;00m credentials \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     credentials, _ \u001b[39m=\u001b[39m google\u001b[39m.\u001b[39mauth\u001b[39m.\u001b[39mdefault(\n\u001b[0;32m    102\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mscopes_kwargs, quota_project_id\u001b[39m=\u001b[39mquota_project_id\n\u001b[0;32m    103\u001b[0m     )\n\u001b[0;32m    104\u001b[0m     \u001b[39m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(credentials, \u001b[39m\"\u001b[39m\u001b[39mwith_gdch_audience\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\auth\\_default.py:648\u001b[0m, in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    640\u001b[0m             _LOGGER\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    641\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mNo project ID could be determined. Consider running \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    642\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m`gcloud config set project` or setting the \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39menvironment variable\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    644\u001b[0m                 environment_vars\u001b[39m.\u001b[39mPROJECT,\n\u001b[0;32m    645\u001b[0m             )\n\u001b[0;32m    646\u001b[0m         \u001b[39mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[1;32m--> 648\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "from google.cloud import speech\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    client = speech.SpeechClient()\n",
    "\n",
    "    # Read the audio file\n",
    "    with open(audio_path, \"rb\") as audio_file:\n",
    "        content = audio_file.read()\n",
    "\n",
    "    # Configure audio settings\n",
    "    audio = speech.RecognitionAudio(content=content)\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=44100,\n",
    "        language_code=\"en-US\",\n",
    "    )\n",
    "\n",
    "    # Request transcription\n",
    "    response = client.recognize(config=config, audio=audio)\n",
    "\n",
    "    # Retrieve and format the transcript\n",
    "    transcript = \"\"\n",
    "    for result in response.results:\n",
    "        transcript += result.alternatives[0].transcript\n",
    "\n",
    "    return transcript\n",
    "\n",
    "\n",
    "# Usage\n",
    "audio_url = r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\out.wav\"\n",
    "transcript = transcribe_audio(audio_url)\n",
    "print(\"Transcript:\", transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Transcribing texts #\n",
      "\n",
      " Artificial intelligence is a branch of computer science and engineering that focuses on developing intelligent machines capable of performing quantitative tasks that are traditionally associated with human beings. Data structure is a collection of data elements that are organized in a particular way to facilitate efficient processing of data, different types of data structure or stutter to different kinds of applications depending on the nature of data and the operation that need to be performed. Data planning is a type of artificial intelligence that relies on artificial neural network with multiple layers to process and analyze large amount of data. These networks are trained using algorithms that adjust the weights and biases of the connections between neurons to optimize the performance on a specific task. Machine learning is a process of teaching computer to learn from the data without being explicitly programmed by analyzing and finding patterns in a large dataset. Machine learning algorithm can be used to make predictions or decisions in a variety of applications. Because its core data science is about using tools and techniques to identify patterns, generate insights and make prediction based on data, this involves working with large and complex dataset and using statistical methods and machine learning algorithm to analyze the data and uncover hidden patterns.\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "import torch\n",
    "import whisper\n",
    "from typing import NewType\n",
    "import warnings\n",
    "import timeit\n",
    "from functools import lru_cache\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# path_of_audio = NewType('path_of_i_th_audio',str)\n",
    "\n",
    "def banner(text):\n",
    "    # \"\"\"Display a message when the script is working in the background\"\"\"\n",
    "    print(f\"# {text} #\\n\")\n",
    "\n",
    "\n",
    "def check_device():\n",
    "    \n",
    "    # \"\"\"Check CUDA availability.\"\"\"\n",
    "    if torch.cuda.is_available() == 1:\n",
    "        device = \"cuda\"\n",
    "        \n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        \n",
    "    return device\n",
    "    # return \"cpu\"\n",
    "\n",
    "# \"\"\"Get speech recognition model.\"\"\"\n",
    "# model_name = input(\"Select speech recognition model name (tiny, base, small, medium, large): \")\n",
    "# @lru_cache(maxsize=None)  # Decorator to enable caching\n",
    "def load_model(model_name):\n",
    "    return whisper.load_model(model_name, device=check_device())\n",
    "\n",
    "def convertText(AUDIOFILE):\n",
    "    \n",
    "    # list_temp=[]\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    #choose a mode defaulted\n",
    "    \"\"\"tiny\"\"\"\n",
    "\n",
    "    model_name = \"base\"\n",
    "\n",
    "    banner(\"Transcribing texts\")\n",
    "    model = load_model(model_name)\n",
    "        \n",
    "    # for i in range(0,len(AUDIOFILE)):\n",
    "        \n",
    "    result = model.transcribe(AUDIOFILE)\n",
    "    warnings.resetwarnings()\n",
    "    # print(\"Result: \",result[\"text\"])\n",
    "    return result[\"text\"]\n",
    "        \n",
    "   \n",
    "    \n",
    "    # return list_temp\n",
    "    \n",
    "# measure execution time of my_function\n",
    "print(convertText(r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\out1.mp3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:443\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    446\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:566\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:532\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:627\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 627\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    629\u001b[0m     \u001b[39mif\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:565\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    563\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m    566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[0;32m    154\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    155\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:448\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    446\u001b[0m     \u001b[39m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[39m# there is yet no clean way to get at it from this context.\u001b[39;00m\n\u001b[1;32m--> 448\u001b[0m     \u001b[39mraise\u001b[39;00m ReadTimeoutError(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool, \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mRead timed out.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m BaseSSLError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[39m# FIXME: Is there a better way to differentiate between SSLErrors?\u001b[39;00m\n",
      "\u001b[1;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\test.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#Model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m BertForQuestionAnswering\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#Tokenizer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2208\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2194\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m   2196\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2197\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2206\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2207\u001b[0m     )\n\u001b[1;32m-> 2208\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2210\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2212\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[0;32m   2213\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1326\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1324\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1326\u001b[0m     http_get(\n\u001b[0;32m   1327\u001b[0m         url_to_download,\n\u001b[0;32m   1328\u001b[0m         temp_file,\n\u001b[0;32m   1329\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1330\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1331\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1332\u001b[0m     )\n\u001b[0;32m   1334\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1335\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:538\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[0;32m    528\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m()\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    530\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    531\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    532\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    537\u001b[0m )\n\u001b[1;32m--> 538\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    540\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[39mraise\u001b[39;00m ContentDecodingError(e)\n\u001b[0;32m    821\u001b[0m \u001b[39mexcept\u001b[39;00m ReadTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 822\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e)\n\u001b[0;32m    823\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    824\u001b[0m     \u001b[39mraise\u001b[39;00m RequestsSSLError(e)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#Model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading ()okenizer_config.json: 100%|| 26.0/26.0 [00:00<00:00, 21.2kB/s]\n",
      "Downloading ()lve/main/config.json: 100%|| 757/757 [00:00<?, ?B/s] \n",
      "Downloading ()olve/main/vocab.json: 100%|| 798k/798k [00:01<00:00, 789kB/s]\n",
      "Downloading ()olve/main/merges.txt: 100%|| 456k/456k [00:00<00:00, 619kB/s]\n",
      "Downloading ()cial_tokens_map.json: 100%|| 150/150 [00:00<00:00, 62.2kB/s]\n",
      "Downloading pytorch_model.bin: 100%|| 595M/595M [07:58<00:00, 1.24MB/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argmax(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\extinsion_interview\\test\\test.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/extinsion_interview/test/test.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m start_scores, end_scores \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39mattention_mask)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/extinsion_interview/test/test.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m all_tokens \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(input_ids[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/extinsion_interview/test/test.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m answer_tokens \u001b[39m=\u001b[39m all_tokens[torch\u001b[39m.\u001b[39;49margmax(start_scores) :torch\u001b[39m.\u001b[39margmax(end_scores)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/extinsion_interview/test/test.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m answer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(tokenizer\u001b[39m.\u001b[39mconvert_tokens_to_ids(answer_tokens))\n",
      "\u001b[1;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "\n",
    "text = \"Huggingface has democratized NLP. Huge thanks to Huggingface for this.\"\n",
    "question = \"What has Huggingface done ?\"\n",
    "encoding = tokenizer(question, text, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "\n",
    "# default is local attention everywhere\n",
    "# the forward method will automatically set global attention on question tokens\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "start_scores, end_scores = model(input_ids, attention_mask=attention_mask)\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n",
    "answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "# output => democratized NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading ()lve/main/config.json: 100%|| 443/443 [00:00<00:00, 125kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\test.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X30sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#Model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m BertForQuestionAnswering\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#Tokenizer\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/test.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:2208\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2194\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2195\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m   2196\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2197\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2206\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2207\u001b[0m     )\n\u001b[1;32m-> 2208\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2210\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2211\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2212\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m SAFE_WEIGHTS_NAME:\n\u001b[0;32m   2213\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:120\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    118\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:1326\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[0;32m   1324\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[1;32m-> 1326\u001b[0m     http_get(\n\u001b[0;32m   1327\u001b[0m         url_to_download,\n\u001b[0;32m   1328\u001b[0m         temp_file,\n\u001b[0;32m   1329\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1330\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1331\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1332\u001b[0m     )\n\u001b[0;32m   1334\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1335\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:538\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[0;32m    528\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m()\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    530\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    531\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    532\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    536\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    537\u001b[0m )\n\u001b[1;32m--> 538\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    539\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    540\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:627\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[1;32m--> 627\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    629\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    630\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:566\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    563\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    565\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 566\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\urllib3\\response.py:532\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    530\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    531\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 532\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\http\\client.py:465\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     amt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength\n\u001b[1;32m--> 465\u001b[0m s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mread(amt)\n\u001b[0;32m    466\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s \u001b[39mand\u001b[39;00m amt:\n\u001b[0;32m    467\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    468\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1131\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#Model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9630601\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def compare(sentence1, sentence2):\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    embeddings = model.encode([sentence1, sentence2])\n",
    "    similarity_score1 = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    # similarity_score2 = 1 - cosine([embeddings[0]], [embeddings[1]])\n",
    "    print(similarity_score1)\n",
    "\n",
    "# Example usage\n",
    "# sentence1 = 'Artificial Intelligence (AI) refers to the ability of machines to perform tasks that typically require human-like intelligence, such as learning, reasoning, problem-solving, perception, and natural language processing.'\n",
    "# sentence1 = \"Mount Everest is the highest peak in the world, located in the Himalayas. It stands at an elevation of approximately 8,848 meters (29,029 feet) above sea level. Climbing Mount Everest is a challenging and dangerous feat that requires extensive preparation, physical endurance, and mountaineering skills. Many climbers attempt to conquer Everest each year, braving extreme weather conditions and navigating treacherous terrain. The summit offers breathtaking views and a sense of accomplishment for those who reach the top.\"\n",
    "sentence1 = \"Machine Learning (ML) is a subfield of artificial intelligence that focuses on the development of algorithms and models that allow computers to learn and make predictions or decisions without being explicitly programmed. It involves the study of statistical models and algorithms that enable systems to automatically analyze and interpret complex patterns and relationships in data.In machine learning, computers are trained on large datasets, where they learn from examples and experience to recognize patterns and make informed decisions or predictions. The process involves extracting meaningful features from the data, selecting appropriate algorithms or models, and optimizing them based on the training data. The trained models can then be used to make predictions or take actions on new, unseen data.\"\n",
    "# sentence2 = \"Artificial intelligence is a branch of computer science and engineering that focuses on developing intelligent machines capable of performing quantitative tasks that are traditionally associated with human beings. Artificial intelligence (AI) refers to the development of computer systems that possess the ability to perform tasks that typically require human intelligence. It encompasses a wide range of techniques and methodologies aimed at creating intelligent machines capable of learning, reasoning, problem-solving, perceiving, and processing natural language. AI systems are designed to emulate human-like cognitive abilities and make autonomous decisions or take actions based on data and patterns. By leveraging algorithms, statistical models, and large datasets, AI enables machines to recognize patterns, make predictions, adapt to changing conditions, and interact with humans in a natural and intelligent manner. AI has applications in various domains, including robotics, healthcare, finance, transportation, and many others, where it has the potential to revolutionize industries, improve efficiency, and enhance decision-making processes.\"\n",
    "# sentence1 = \"Artificial intelligence (AI) is a branch of computer science that focuses on developing intelligent machines capable of performing tasks that typically require human intelligence. It involves the study of algorithms and models that enable computers to learn, reason, problem-solve, and understand natural language. AI has various applications in fields like robotics, healthcare, finance, and transportation, where it aims to revolutionize industries and improve efficiency.\"\n",
    "sentence2 = \"Machine learning (ML) is a subset of AI that deals with the development of algorithms and models that enable computers to learn and make predictions based on data. It involves training computers on large datasets to recognize patterns and make informed decisions without being explicitly programmed. ML has applications in areas like image and speech recognition, recommendation systems, and fraud detection, where it plays a vital role in automating tasks and extracting meaningful insights from data.\"\n",
    "compare(sentence1, sentence2)\n",
    "# print(\"Similarity score:\", similarity_score1,similarity_score2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean Distance: 21.790569305419915\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def compare(sentence1, sentence2):\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    embeddings = model.encode([sentence1, sentence2])\n",
    "    \n",
    "    # Calculate Euclidean distance\n",
    "    distance = euclidean(embeddings[0], embeddings[1])\n",
    "\n",
    "    # Calculate similarity score\n",
    "    similarity_score = 1 / (1 + distance)  # Inverse of the distance\n",
    "\n",
    "    # Alternatively, you can normalize the distance between 0 and 1\n",
    "    # using min-max normalization\n",
    "    min_distance = 0.0  # Minimum possible distance\n",
    "    max_distance = 10.0  # Maximum possible distance\n",
    "\n",
    "    normalized_distance = (distance - min_distance) / (max_distance - min_distance)\n",
    "\n",
    "    # Transform the normalized distance to a similarity score between 0 and 100\n",
    "    similarity_score = 100 * (1 - normalized_distance)\n",
    "    \n",
    "    return similarity_score*3.3\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# sentence1 = 'Artificial Intelligence (AI) refers to the ability of machines to perform tasks that typically require human-like intelligence, such as learning, reasoning, problem-solving, perception, and natural language processing.'\n",
    "# sentence2 = \"Mount Everest is the highest peak in the world, located in the Himalayas. It stands at an elevation of approximately 8,848 meters (29,029 feet) above sea level. Climbing Mount Everest is a challenging and dangerous feat that requires extensive preparation, physical endurance, and mountaineering skills. Many climbers attempt to conquer Everest each year, braving extreme weather conditions and navigating treacherous terrain. The summit offers breathtaking views and a sense of accomplishment for those who reach the top.\"\n",
    "sentence2 = \"Artificial intelligence is a branch of computer science and engineering that focuses on developing intelligent machines capable of performing quantitative tasks that are traditionally associated with human beings. Artificial intelligence (AI) refers to the development of computer systems that possess the ability to perform tasks that typically require human intelligence. It encompasses a wide range of techniques and methodologies aimed at creating intelligent machines capable of learning, reasoning, problem-solving, perceiving, and processing natural language. AI systems are designed to emulate human-like cognitive abilities and make autonomous decisions or take actions based on data and patterns. By leveraging algorithms, statistical models, and large datasets, AI enables machines to recognize patterns, make predictions, adapt to changing conditions, and interact with humans in a natural and intelligent manner. AI has applications in various domains, including robotics, healthcare, finance, transportation, and many others, where it has the potential to revolutionize industries, improve efficiency, and enhance decision-making processes.\"\n",
    "sentence2 = \"Machine learning (ML) is a subset of AI that deals with the development of algorithms and models that enable computers to learn and make predictions based on data. It involves training computers on large datasets to recognize patterns and make informed decisions without being explicitly programmed. ML has applications in areas like image and speech recognition, recommendation systems, and fraud detection, where it plays a vital role in automating tasks and extracting meaningful insights from data.\"\n",
    "\n",
    "similarity= compare(sentence1, sentence2)\n",
    "# print(\"Cosine Similarity:\", similarity_cosine)\n",
    "print(\"Euclidean Distance:\", similarity if similarity>0 else abs(similarity)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.80075985e-01  7.46048987e-01  8.99167001e-01 -1.11929864e-01\n",
      "   9.99872833e-02 -6.65144801e-01 -1.31204531e-01  2.94809908e-01\n",
      "   3.32077533e-01 -2.26748407e-01  1.74631655e-01  6.98763072e-01\n",
      "   1.07620746e-01  7.90148914e-01 -6.92231119e-01  9.93288085e-02\n",
      "  -8.81605029e-01 -4.83663261e-01  1.23142436e-01  2.55781144e-01\n",
      "  -4.30965841e-01 -6.84490204e-01  2.96399087e-01  6.50981009e-01\n",
      "  -1.38950218e-02  2.50701725e-01 -6.77251160e-01  3.52156669e-01\n",
      "  -9.63392496e-01  1.99173778e-01 -8.31316948e-01 -1.62037894e-01\n",
      "   1.04588248e-01 -8.75517011e-01 -7.75868475e-01  9.92624164e-01\n",
      "   5.93554676e-01  6.52876124e-02 -9.68944728e-02  2.71096349e-01\n",
      "  -5.14553905e-01 -7.82622844e-02  2.01307952e-01  2.91496664e-01\n",
      "  -1.42707038e+00 -5.10521650e-01 -1.38222396e+00  4.41120327e-01\n",
      "   1.43147483e-01  1.84224322e-01 -3.64681035e-01  4.95001674e-01\n",
      "  -3.44197005e-01 -6.68913126e-01 -2.99958169e-01  6.79977834e-01\n",
      "   1.20468192e-01 -1.71773791e+00 -1.20194882e-01 -1.01770051e-01\n",
      "  -4.30761516e-01  2.06781670e-01  4.00595665e-01  3.05001740e-03\n",
      "  -2.46523947e-01 -1.87855378e-01  9.07380044e-01 -2.14899451e-01\n",
      "  -1.41343307e+00 -7.65291750e-01  6.94294944e-02 -8.39342415e-01\n",
      "  -4.68807906e-01 -1.61124334e-01 -8.26031864e-01 -4.25836593e-01\n",
      "   1.18562385e-01  2.74842143e-01  6.77472174e-01  4.00354058e-01\n",
      "  -2.07506806e-01 -2.53272150e-02  6.43282294e-01  1.74141824e-01\n",
      "   8.63449633e-01 -1.40678301e-01  5.92090368e-01  2.47974679e-01\n",
      "  -5.42574584e-01  6.41081408e-02  1.95232257e-01 -6.96866453e-01\n",
      "   5.72848380e-01 -4.04079616e-01 -5.10629267e-02 -3.41708124e-01\n",
      "   1.15124740e-01 -4.47313696e-01 -9.32007790e-01  4.71554399e-01\n",
      "  -3.20181757e-01 -8.47631022e-02 -2.12232724e-01 -7.51725808e-02\n",
      "  -4.13124174e-01  7.02738523e-01  1.12472787e-01 -6.02916598e-01\n",
      "   3.54916066e-01  1.78912170e-02  5.81292212e-01 -5.06954968e-01\n",
      "  -6.01322711e-01  3.43405157e-01 -4.67988282e-01  4.43445981e-01\n",
      "  -6.87579453e-01  9.47114229e-01  1.06897676e+00 -1.23420067e-01\n",
      "   3.79414767e-01 -2.15638533e-01  2.09091511e-02  9.52068791e-02\n",
      "   9.53785181e-02 -3.63296896e-01 -1.15315817e-01  5.25894463e-01\n",
      "  -1.66070253e-01 -1.34193942e-01  4.51162279e-01  7.23708987e-01\n",
      "   7.82305956e-01  1.40762985e-01  4.21017379e-01  9.29254293e-01\n",
      "   6.88972026e-02 -5.35019159e-01 -9.81803834e-01 -6.90076547e-03\n",
      "  -1.36756167e-01 -1.79055840e-01  8.77818823e-01 -4.45250124e-01\n",
      "  -1.92528948e-01 -7.83386111e-01  7.74642155e-02 -2.95708477e-01\n",
      "  -7.00985849e-01 -2.26695448e-01 -6.37819827e-01  2.09079295e-01\n",
      "  -2.12707782e+00 -4.87062901e-01  8.25200856e-01  9.81782258e-01\n",
      "  -1.03009629e+00 -2.90160090e-01  2.75818706e-01  5.51491201e-01\n",
      "  -3.13966900e-01  1.10527158e+00 -1.46488106e+00  3.28814983e-01\n",
      "  -1.89705029e-01 -9.04079437e-01  1.38778162e+00 -3.41887265e-01\n",
      "  -2.00734317e-01  2.45869588e-02  5.40585518e-01  7.96439946e-01\n",
      "   8.24309409e-01 -1.82525679e-01  1.63165823e-01 -6.57415152e-01\n",
      "   1.07077038e+00 -5.52735031e-01 -7.60154843e-01 -7.20513284e-01\n",
      "   1.73378259e-01 -3.30135971e-01 -4.14824158e-01  4.09442127e-01\n",
      "  -2.41204053e-01 -7.92888522e-01  9.46505249e-01 -4.39568907e-02\n",
      "   2.27178231e-01 -4.79385048e-01  7.18671143e-01  5.49470901e-01\n",
      "  -2.25686938e-01 -5.81619501e-01  2.28047203e-02  6.09886169e-01\n",
      "  -5.08200824e-01  1.45038962e+00 -6.69942975e-01  9.76005435e-01\n",
      "  -3.01323086e-01  3.09970111e-01  6.51482865e-02 -5.72983682e-01\n",
      "   2.53335834e-01 -5.12553334e-01 -4.87695277e-01 -2.13597611e-01\n",
      "  -1.67214394e+00 -6.06944919e-01 -4.47708815e-01  1.06503046e+00\n",
      "   7.55712271e-01  9.75083634e-02  4.35135454e-01  3.04344654e-01\n",
      "   1.40261143e-01  4.84324902e-01  4.11380589e-01  4.15246606e-01\n",
      "   8.64937365e-01  2.32286021e-01 -1.91720929e-02  1.10349524e+00\n",
      "  -6.39030710e-02  8.46441627e-01  2.08938465e-01 -2.15029910e-01\n",
      "   3.21080089e-01  1.07552266e+00  4.77229625e-01  2.13718135e-02\n",
      "   4.28112894e-01 -3.67393643e-01  2.38859951e-01 -3.65569025e-01\n",
      "  -1.03213377e-01  1.24689646e-01  8.11453819e-01  3.35284114e-01\n",
      "   2.85733491e-01  1.33490682e+00  1.11707523e-01  4.21555072e-01\n",
      "   1.87969953e-01 -7.16725588e-01  4.29133251e-02  3.03291887e-01\n",
      "   1.27176449e-01 -2.91078202e-02 -7.83779800e-01 -2.10448518e-01\n",
      "   6.96452931e-02 -1.21740520e+00  3.36919487e-01 -3.95299345e-01\n",
      "  -4.90986198e-01 -4.37094897e-01 -1.13858342e+00  2.69624203e-01\n",
      "   6.56059831e-02  8.19980912e-03 -9.95576441e-01  3.98305088e-01\n",
      "  -6.65772557e-02 -7.13660181e-01 -2.38642156e-01 -2.45518297e-01\n",
      "   6.31742775e-02 -1.77397594e-01 -4.54458416e-01 -5.10687172e-01\n",
      "  -3.21250334e-02  2.45687008e-01  9.62202966e-01 -6.91768110e-01\n",
      "  -5.27623296e-01 -4.17339146e-01  8.05348679e-02  1.44890726e-01\n",
      "  -4.19544846e-01  4.49928552e-01 -5.08797228e-01  1.00922391e-01\n",
      "   5.12932956e-01  6.27563298e-01 -3.87067735e-01 -1.44008219e-01\n",
      "  -3.39247212e-02 -5.92323057e-02 -2.48882715e-02 -2.61314183e-01\n",
      "  -1.07574260e+00 -4.68898475e-01  5.48127890e-01 -8.41647863e-01\n",
      "   7.60025620e-01  6.60878569e-02  2.61370540e-01 -2.42343932e-01\n",
      "  -1.04097486e+00  8.13440979e-01 -1.40573633e+00 -6.32980406e-01\n",
      "   2.76805848e-01 -5.99700511e-01  1.39096037e-01  2.74772942e-01\n",
      "   1.12895533e-01 -1.17395353e+00  2.51422465e-01 -6.20175123e-01\n",
      "  -2.30993819e-03  4.69823666e-02 -7.91156232e-01  1.32366329e-01\n",
      "  -3.14429224e-01  2.70541608e-01  8.17799866e-01 -5.87337732e-01\n",
      "  -5.57603419e-01  1.23343632e-01  1.68064430e-01  9.02432680e-01\n",
      "   5.72104514e-01  1.12155400e-01 -1.09220438e-01  4.05462444e-01\n",
      "  -3.47702861e-01 -1.64182723e-01  6.74383044e-01  2.56645143e-01\n",
      "  -2.65341699e-01  1.05606043e+00 -6.95906654e-02  7.64667332e-01\n",
      "  -1.42732191e+00  1.90484971e-01 -7.68410936e-02  1.57249779e-01\n",
      "  -4.82545555e-01  6.40135407e-02 -1.57691076e-01 -3.20658207e-01\n",
      "   4.80449170e-01 -1.50108963e-01  1.76289529e-01 -1.13660246e-01\n",
      "   3.21787000e-01 -1.72548503e-01  7.10948646e-01 -1.09946668e+00\n",
      "   4.69147503e-01 -7.10890353e-01 -1.46656656e+00  4.09042180e-01\n",
      "  -1.90067708e-01  2.25701518e-02 -4.40304816e-01  1.44703656e-01\n",
      "   9.17053223e-01  4.72922623e-01  3.99866588e-02 -8.44726443e-01\n",
      "   5.53363562e-01 -3.87432992e-01 -2.81478494e-01  5.38880005e-02\n",
      "   5.40231645e-01  2.47818574e-01 -1.16669148e-01 -3.70472729e-01\n",
      "  -5.61463654e-01 -2.18348950e-01 -5.21927118e-01 -8.04422140e-01\n",
      "  -8.22071195e-01 -2.04165444e-01  3.98418233e-02  4.83975500e-01\n",
      "  -1.68168351e-01 -2.52746046e-01 -8.71243402e-02 -3.08831669e-02\n",
      "  -3.68366465e-02  3.27344120e-01  4.85281974e-01  7.24930644e-01\n",
      "   4.91388328e-02 -2.49129966e-01 -3.69892120e-01  2.43476778e-01\n",
      "  -2.77495414e-01 -6.87904000e-01 -7.38594010e-02  1.62256192e-02\n",
      "  -8.72244000e-01  2.03629695e-02  1.38345316e-01 -7.09808350e-01\n",
      "   1.21875666e-02  1.02248538e+00  1.45287549e+00 -1.24515080e+00\n",
      "  -8.54457170e-02  1.00375640e+00  7.18355715e-01 -5.67385972e-01\n",
      "   3.68733555e-01 -9.48279977e-01 -4.11090672e-01 -2.24804133e-01\n",
      "   1.32043183e+00  6.64651811e-01 -4.75509822e-01 -8.56411219e-01\n",
      "   2.29806468e-01 -6.58353031e-01 -1.24323094e+00  5.03352761e-01\n",
      "  -6.83606446e-01  1.53258666e-01  3.17032963e-01  1.69284567e-01\n",
      "   2.47404158e-01  5.70592463e-01  4.91724396e-03 -6.17087722e-01\n",
      "  -9.17260766e-01  6.21468611e-02 -4.44176793e-01  1.09187317e+00\n",
      "  -3.20266455e-01  2.64809936e-01  7.69540787e-01  7.40203083e-01\n",
      "   4.69394863e-01 -1.13915741e+00  7.48420596e-01 -5.34451544e-01\n",
      "  -8.73450041e-02 -6.20812416e-01  4.58307654e-01  2.89317399e-01\n",
      "   5.53758323e-01  6.90127075e-01 -9.00477991e-02  7.69334674e-01\n",
      "   2.73305297e-01  1.40582263e-01  9.02058840e-01 -5.74126422e-01\n",
      "   1.01230212e-01 -1.06217913e-01 -2.80707538e-01  9.93135989e-01\n",
      "  -6.59153759e-01 -2.18460724e-01 -1.05292225e+00 -5.98369360e-01\n",
      "  -1.88480332e-01 -2.27525204e-01  8.77727866e-01 -5.20864725e-01\n",
      "   6.01016104e-01  1.25945485e+00 -3.59350771e-01 -6.87756956e-01\n",
      "   4.53094929e-01  1.49783455e-02  4.64237779e-01  1.96256731e-02\n",
      "  -8.33755493e-01  6.59662127e-01  4.99197841e-01 -1.74410403e-01\n",
      "  -3.20094973e-01  7.78781891e-01  3.65491241e-01 -1.33477688e-01\n",
      "  -2.97452241e-01  2.73389250e-01 -1.37728274e-01 -4.73189354e-01\n",
      "   1.35387257e-01 -2.11688340e-01 -6.36470169e-02  7.43963242e-01\n",
      "   4.81119938e-02 -3.09414983e-01 -2.08441436e-01  5.41133821e-01\n",
      "  -1.04571795e+00 -1.72207013e-01  1.91261038e-01 -1.02225053e+00\n",
      "  -7.07878768e-01  2.20691741e-01  4.24459666e-01  1.00481355e+00\n",
      "   2.05109626e-01  4.35360730e-01  7.37038031e-02  3.28219905e-02\n",
      "   2.80097902e-01  1.42401829e-01 -6.39027804e-02 -7.37772882e-01\n",
      "   3.70068550e-01  5.33393085e-01 -1.42598644e-01 -1.60478339e-01\n",
      "   1.05806828e+00 -5.81580959e-02 -6.42948151e-01  1.12909806e+00\n",
      "  -2.50378549e-01  7.30046332e-01 -3.65986377e-01 -1.27431080e-01\n",
      "   9.78620872e-02 -3.12942296e-01  3.18041056e-01 -1.25115812e+00\n",
      "   6.29082084e-01  1.27809477e+00  6.64224267e-01  3.15339237e-01\n",
      "   1.27079010e-01 -7.86215246e-01 -2.55822748e-01  3.88179004e-01\n",
      "  -5.40759623e-01 -6.52464569e-01  1.28127337e-02  1.19192457e+00\n",
      "  -6.35564506e-01 -6.59992456e-01  2.21328735e-01 -2.16403693e-01\n",
      "   9.40025449e-01  5.40558815e-01  4.36241359e-01 -3.65035057e-01\n",
      "  -9.53621149e-01  3.88679832e-01  5.37412286e-01 -3.98120373e-01\n",
      "  -7.04056680e-01 -7.77649432e-02 -8.06418777e-01  4.86418545e-01\n",
      "   8.31306994e-01  5.61289430e-01 -8.10162544e-01  3.80894840e-01\n",
      "  -8.34646344e-01  2.44981915e-01 -3.92054021e-01  9.31609392e-01\n",
      "   8.19813013e-01  8.43266785e-01 -5.52334487e-01  3.11978728e-01\n",
      "   2.24079788e-01 -8.48612428e-01 -3.63641739e-01 -2.29334190e-01\n",
      "  -6.21085584e-01  5.10551214e-01 -5.34460604e-01 -6.13159776e-01\n",
      "   2.46384799e-01  1.59111178e+00 -1.09893739e+00 -5.29865682e-01\n",
      "   1.35249168e-01 -1.14532900e+00 -2.23334223e-01  3.05225790e-01\n",
      "  -4.52889383e-01  5.57049751e-01 -2.11937934e-01  4.31850731e-01\n",
      "  -1.46599278e-01 -6.38794482e-01 -1.79678530e-01  2.11695060e-01\n",
      "  -2.97130227e-01 -9.76281881e-01  1.85583755e-01 -5.49944699e-01\n",
      "   6.13414943e-01  1.59145862e-01  1.00802723e-02 -4.59116161e-01\n",
      "   9.51667964e-01  4.07434106e-01  4.42010224e-01  3.25191408e-01\n",
      "   6.13964736e-01 -8.82021010e-01 -1.32806611e+00 -3.36057156e-01\n",
      "  -1.38377801e-01 -1.91878214e-01 -4.10806090e-01  7.90024102e-02\n",
      "   2.88211137e-01  1.87653482e-01  2.88753986e-01 -1.08207858e+00\n",
      "   4.23200041e-01 -5.18274829e-02  2.77438276e-02 -6.84756637e-01\n",
      "  -6.01071775e-01 -2.82739252e-01 -1.74133554e-02  6.18749373e-02\n",
      "   3.51094931e-01  2.26806834e-01 -4.75269467e-01  3.09305694e-02\n",
      "   1.00146282e+00 -1.23407602e+00  2.35548556e-01  1.50981233e-01\n",
      "  -1.85616210e-01  2.75436401e-01  8.18527460e-01  4.06380445e-01\n",
      "   4.68772501e-01 -2.39393152e-02 -9.64470301e-03  5.25283754e-01\n",
      "  -1.42911232e+00  7.04130605e-02 -3.87212485e-02  5.11230111e-01\n",
      "  -1.38452220e+00  8.89841199e-01  2.81189382e-01 -4.51522678e-01\n",
      "  -6.99669197e-02  3.32231879e-01  5.15710890e-01  4.26898003e-01\n",
      "   1.15739608e+00  4.25603658e-01  2.89169937e-01  5.10268986e-01\n",
      "   1.41217202e-01  8.16301405e-02  9.11411420e-02 -2.98670083e-01\n",
      "  -9.37656939e-01  1.08347252e-01  6.60720289e-01  4.36673224e-01\n",
      "   2.24545345e-01  2.90245473e-01  8.40063453e-01 -1.10009444e+00\n",
      "   1.32772386e+00 -1.82388410e-01 -2.50358373e-01  2.34372661e-01\n",
      "   1.51598498e-01 -3.68623674e-01  1.00164831e+00 -1.96801737e-01\n",
      "  -7.66698539e-01  3.49254727e-01  1.03487659e+00 -5.09133875e-01\n",
      "   6.36476874e-01 -2.26108104e-01 -6.62051082e-01 -1.32825851e+00\n",
      "  -7.34639525e-01 -8.92027915e-01 -6.65982589e-05 -9.54945564e-01\n",
      "   3.55426371e-01  3.48104089e-01 -8.08886588e-01  1.83736220e-01\n",
      "   7.99303591e-01  1.71448791e+00  2.43597969e-01  2.25528181e-01\n",
      "   3.15346196e-02  3.71955812e-01 -3.37683141e-01 -8.17944705e-02\n",
      "  -2.73701221e-01  1.50666520e-01  3.39550704e-01 -3.01195588e-02\n",
      "  -1.32265598e-01  1.26159799e+00  3.92633945e-01 -8.10757637e-01\n",
      "  -5.78759611e-01 -9.71880406e-02 -4.00009453e-01  2.38922372e-01\n",
      "   6.11667633e-02 -8.77642110e-02 -1.62007868e-01  1.30763221e+00\n",
      "   1.62282944e-01  1.81222111e-01 -9.34249997e-01 -6.42187715e-01\n",
      "   1.42141297e-01  1.13676891e-01 -1.83794498e-01  1.86907366e-01\n",
      "  -4.33240503e-01 -6.74938917e-01 -6.67641997e-01 -6.09567702e-01\n",
      "   6.05616331e-01  3.07549298e-01  7.08776653e-01  2.44257197e-01\n",
      "  -2.51015812e-01  1.35904759e-01  1.59745133e+00 -8.49826813e-01\n",
      "   2.27751344e-01  3.64066809e-01  4.37265217e-01 -3.40348333e-02\n",
      "   4.79170159e-02 -3.74329865e-01 -4.81514186e-01 -7.05480576e-01\n",
      "  -9.98946577e-02  1.40654877e-01  2.07113344e-02 -1.65988192e-01\n",
      "  -2.63182912e-03  1.30796671e-01  1.27015546e-01 -3.21190268e-01\n",
      "  -9.03084800e-02  4.48724627e-01  6.23776793e-01 -1.54030228e+00\n",
      "   4.40886378e-01  8.21799219e-01 -2.71965444e-01  3.22469085e-01\n",
      "  -4.19865668e-01 -6.33923650e-01 -5.93827367e-01 -1.95953831e-01\n",
      "  -1.08571470e+00 -1.07554317e+00 -2.13499606e-01 -6.31553233e-01\n",
      "   4.41305071e-01 -6.79798007e-01 -1.39802349e+00 -1.13373645e-01]]\n",
      "[22, 20]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from time import sleep\n",
    "import random\n",
    "from math import ceil\n",
    "def calculate_similarity(sentences_list,sentence):\n",
    "    # Encode sentences and obtain embeddings\n",
    "    \"\"\" Can even use 'bert-base-nli-stsb-mean-tokens' or 'bert-base-nli-max-tokens'\"\"\"\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')        #180 97 -366 218 -124 -309\n",
    "    # model = SentenceTransformer('bert-base-nli-stsb-mean-tokens') #177 79 -384 214 -108 -352\n",
    "    # model = SentenceTransformer('bert-base-nli-max-tokens')       #149 65 -307 157 -109 -277  \"\"\" Reject this model \"\"\"\n",
    "\n",
    "    sentence_embeddings = model.encode([sentence])\n",
    "    sentences_embeddings = model.encode(sentences_list)\n",
    "    # print(sentence_embeddings)\n",
    "    \n",
    "    # # Calculate Cosine similarity\n",
    "    # cos_similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    # cosine_similarity_score = (ceil(101 * cos_similarity)) if cos_similarity > 0 else 0\n",
    "    \n",
    "    # # Calculate Euclidean distance\n",
    "    # euclidean_distance = euclidean(embeddings[0], embeddings[1])\n",
    "    # euclidean_similarity_score = ceil(calculate_similarity_score(abs(euclidean_distance)))\n",
    "    \n",
    "\n",
    "    # # Calculate cosine similarity\n",
    "    # cos_similarity = util.pytorch_cos_sim(embeddings[0], embeddings[1])\n",
    "    # cosine_similarity_score2 = ceil(cos_similarity.item() * 100) if  ceil(cos_similarity.item() * 100) > 0 else 0\n",
    "    \n",
    "    # print(f\"Similarity Score = {euclidean_similarity_score} , {cosine_similarity_score} , {cosine_similarity_score2}\")\n",
    "    \n",
    "    # weighted_similarity = ceil(euclidean_similarity_score * .70 + cosine_similarity_score * .15 + cosine_similarity_score2*.15)\n",
    "    # print(f\"Final similarity is {weighted_similarity}\")\n",
    "    similarity_score = []\n",
    "\n",
    "    for embedding in sentences_embeddings:\n",
    "        # Calculate cosine similarity\n",
    "        cos_similarity = util.pytorch_cos_sim(sentence_embeddings[0], embedding)\n",
    "        similarity_score_cos = ceil(cos_similarity.item() * 100) if  ceil(cos_similarity.item() * 100) > 0 else 0\n",
    "\n",
    "        # Calculate Euclidean distance\n",
    "        euclidean_distance = euclidean(sentence_embeddings[0], embedding)\n",
    "        similarity_score_euc = ceil(calculate_similarity_score(abs(euclidean_distance)))\n",
    "        \n",
    "        cosi_similarity = cosine_similarity([sentence_embeddings[0]], [embedding])[0][0]\n",
    "        cosine_similarity_score = (ceil(101 * cosi_similarity)) if cosi_similarity > 0 else 0\n",
    "\n",
    "        # Combine the similarity scores (adjust the weights as desired)\n",
    "        similarity_score.append(ceil(0.15 * similarity_score_cos + 0.70 * similarity_score_euc + 0.15 * cosine_similarity_score))\n",
    "\n",
    "    print(similarity_score)\n",
    "    return max(similarity_score)    \n",
    "    \n",
    "\n",
    "def calculate_similarity_score(distance):\n",
    "    # Normalize the distance between 0 and 1\n",
    "    min_distance = 0.0  # Minimum possible distance\n",
    "    max_distance = 10.0  # Maximum possible distance\n",
    "\n",
    "    normalized_distance = (distance - min_distance) / (max_distance - min_distance)\n",
    "\n",
    "    # Transform the normalized distance to a similarity score between 0 and 100\n",
    "    similarity = 100 * (1 - normalized_distance) * 3.3\n",
    "    # similarity = 100 * (1 - normalized_distance)\n",
    "\n",
    "    return similarity if similarity > 0 and similarity < 100 else random.randint(97,100) if similarity > 100 else 0\n",
    "    # return similarity\n",
    "\n",
    "# sentence1 = 'Artificial Intelligence (AI) refers to the ability of machines to perform tasks that typically require human-like intelligence, such as learning, reasoning, problem-solving, perception, and natural language processing.'\n",
    "# sentence2 = \"Artificial intelligence is a branch of computer science and engineering that focuses on developing intelligent machines capable of performing quantitative tasks that are traditionally associated with human beings. Artificial intelligence (AI) refers to the development of computer systems that possess the ability to perform tasks that typically require human intelligence. It encompasses a wide range of techniques and methodologies aimed at creating intelligent machines capable of learning, reasoning, problem-solving, perceiving, and processing natural language. AI systems are designed to emulate human-like cognitive abilities and make autonomous decisions or take actions based on data and patterns. By leveraging algorithms, statistical models, and large datasets, AI enables machines to recognize patterns, make predictions, adapt to changing conditions, and interact with humans in a natural and intelligent manner. AI has applications in various domains, including robotics, healthcare, finance, transportation, and many others, where it has the potential to revolutionize industries, improve efficiency, and enhance decision-making processes.\"\n",
    "sentence1 = [\"Machine learning (ML) is a subset of AI that deals with the development of algorithms and models that enable computers to learn and make predictions based on data. It involves training computers on large datasets to recognize patterns and make informed decisions without being explicitly programmed. ML has applications in areas like image and speech recognition, recommendation systems, and fraud detection, where it plays a vital role in automating tasks and extracting meaningful insights from data.\",'Machine learning (ML) is a subset of artificial intelligence (AI) that focuses on the development of algorithms and models that enable computers to learn and make predictions based on data. It involves training machines on large datasets to recognize patterns, extract insights, and make informed decisions without being explicitly programmed.']\n",
    "# sentence2 = \"Mount Everest is the highest peak in the world, located in the Himalayas. It stands at an elevation of approximately 8,848 meters (29,029 feet) above sea level. Climbing Mount Everest is a challenging and dangerous feat that requires extensive preparation, physical endurance, and mountaineering skills. Many climbers attempt to conquer Everest each year, braving extreme weather conditions and navigating treacherous terrain. The summit offers breathtaking views and a sense of accomplishment for those who reach the top.\"\n",
    "# sentence2 = \"\"\n",
    "sentence2 = \"Machine Learning has numerous applications across various domains, including image and speech recognition, natural language processing, recommendation systems, and fraud detection. It has revolutionized industries and transformed the way businesses operate. With ML, companies can automate tasks, improve efficiency, and gain valuable insights from their data.\"\n",
    "# sentence2 = 'Machine learning (ML) is a subset of artificial intelligence (AI) that focuses on the development of algorithms and models that enable computers to learn and make predictions based on data. It involves training machines on large datasets to recognize patterns, extract insights, and make informed decisions without being explicitly programmed.'\n",
    "# sentence1 = \"My name is Rahul\"\n",
    "# sentence2 = \"Rahul is not my name\"\n",
    "calculate_similarity(sentence1, sentence2)\n",
    "# \"\"\"Succesfull test\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def calculate_similarity(sentence1, sentence2):\n",
    "    # Load pre-trained BERT model and tokenizer\n",
    "    model_name = 'bert-base-uncased'\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize the sentences\n",
    "    tokens1 = tokenizer.tokenize(sentence1)\n",
    "    tokens2 = tokenizer.tokenize(sentence2)\n",
    "\n",
    "    # Add special tokens and obtain input IDs\n",
    "    encoded_input = tokenizer.encode_plus(tokens1, tokens2, add_special_tokens=True, padding='longest', truncation=True)\n",
    "    input_ids = torch.tensor(encoded_input['input_ids']).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Forward pass through the BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # Extract the embedding for the [CLS] token\n",
    "\n",
    "    # Calculate cosine similarity between the embeddings\n",
    "    similarity_score = torch.nn.functional.cosine_similarity(embeddings, embeddings, dim=1)\n",
    "\n",
    "    return similarity_score.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999998211860657"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = 'Artificial Intelligence (AI) refers to the ability of machines to perform tasks that typically require human-like intelligence, such as learning, reasoning, problem-solving, perception, and natural language processing.'\n",
    "# sentence2 = \"Artificial intelligence is a branch of computer science and engineering that focuses on developing intelligent machines capable of performing quantitative tasks that are traditionally associated with human beings. Artificial intelligence (AI) refers to the development of computer systems that possess the ability to perform tasks that typically require human intelligence. It encompasses a wide range of techniques and methodologies aimed at creating intelligent machines capable of learning, reasoning, problem-solving, perceiving, and processing natural language. AI systems are designed to emulate human-like cognitive abilities and make autonomous decisions or take actions based on data and patterns. By leveraging algorithms, statistical models, and large datasets, AI enables machines to recognize patterns, make predictions, adapt to changing conditions, and interact with humans in a natural and intelligent manner. AI has applications in various domains, including robotics, healthcare, finance, transportation, and many others, where it has the potential to revolutionize industries, improve efficiency, and enhance decision-making processes.\"\n",
    "# sentence2 = \"Machine learning (ML) is a subset of AI that deals with the development of algorithms and models that enable computers to learn and make predictions based on data. It involves training computers on large datasets to recognize patterns and make informed decisions without being explicitly programmed. ML has applications in areas like image and speech recognition, recommendation systems, and fraud detection, where it plays a vital role in automating tasks and extracting meaningful insights from data.\"\n",
    "sentence2 = \"Mount Everest is the highest peak in the world, located in the Himalayas. It stands at an elevation of approximately 8,848 meters (29,029 feet) above sea level. Climbing Mount Everest is a challenging and dangerous feat that requires extensive preparation, physical endurance, and mountaineering skills. Many climbers attempt to conquer Everest each year, braving extreme weather conditions and navigating treacherous terrain. The summit offers breathtaking views and a sense of accomplishment for those who reach the top.\"\n",
    "# sentence2 = \"\"\n",
    "calculate_similarity(sentence1,sentence2)\n",
    "\n",
    "# \"\"\"Failure\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055061\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def calculate_similarity(sentence1, sentence2):\n",
    "    # Load the Universal Sentence Encoder\n",
    "    use_model = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "    # Encode the sentences\n",
    "    embeddings = use_model([sentence1, sentence2])\n",
    "\n",
    "    # Calculate cosine similarity score\n",
    "    similarity_score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "\n",
    "    return similarity_score\n",
    "\n",
    "# Example usage+-\n",
    "# sentence1 = 'Artificial Intelligence (AI) refers to the ability of machines to perform tasks that typically require human-like intelligence, such as learning, reasoning, problem-solving, perception, and natural language processing.'\n",
    "# sentence2 = \"Machine learning (ML) is a subset of AI that deals with the development of algorithms and models that enable computers to learn and make predictions based on data. It involves training computers on large datasets to recognize patterns and make informed decisions without being explicitly programmed. ML has applications in areas like image and speech recognition, recommendation systems, and fraud detection, where it plays a vital role in automating tasks and extracting meaningful insights from data.\"\n",
    "sentence1 = \"My name is Rahul\"\n",
    "sentence2 = \"Rahul is not my name\"\n",
    "similarity_score = calculate_similarity(sentence1, sentence2)\n",
    "print(similarity_score)\n",
    "\n",
    "\"\"\"Failure\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(sentence, sentences_list):\n",
    "    # Encode sentences and obtain embeddings\n",
    "    model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "    sentence_embeddings = model.encode([sentence])\n",
    "    sentences_embeddings = model.encode(sentences_list)\n",
    "\n",
    "    max_similarity_score = 0\n",
    "\n",
    "    for embedding in sentences_embeddings:\n",
    "        # Calculate cosine similarity\n",
    "        cos_similarity = util.pytorch_cos_sim(sentence_embeddings[0], embedding)\n",
    "        similarity_score_cos = cos_similarity.item()\n",
    "\n",
    "        # Calculate Euclidean distance\n",
    "        euclidean_distance = euclidean(sentence_embeddings[0], embedding)\n",
    "        similarity_score_euc = 1 / (1 + euclidean_distance)\n",
    "\n",
    "        # Combine the similarity scores (adjust the weights as desired)\n",
    "        similarity_score = 0.7 * similarity_score_cos + 0.3 * similarity_score_euc\n",
    "\n",
    "        if similarity_score > max_similarity_score:\n",
    "            max_similarity_score = similarity_score\n",
    "\n",
    "    return max_similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(r\"D:\\Projects and codes\\interview\\resources\")\n",
    "\n",
    "from NLP_Transformer import SimilarityCalculator\n",
    "\n",
    "obj = SimilarityCalculator()\n",
    "\n",
    "sentence1 = ['Machine learning is a branch of artificial intelligence that involves developing algorithms and models that can learn from data and make predictions or decisions without being explicitly programmed. Machine learning algorithms use mathematical and statistical techniques to analyze and find patterns within large datasets. There are three main types of machine learning: supervised, unsupervised, and reinforcement learning.', 'Machine learning is the process of teaching computers to learn from data, without being explicitly programmed. By analyzing and finding patterns in large datasets, machine learning algorithms can be used to make predictions or decisions in a variety of applications.', 'At its core, machine learning involves using statistical and mathematical techniques to enable computers to learn and make decisions based on data. This has a wide range of practical applications, from speech recognition and image analysis to fraud detection and personalized recommendations.', 'Machine learning is a subset of artificial intelligence that uses mathematical and statistical techniques to analyze data and recognize patterns, enabling computers to make decisions and predictions without being explicitly programmed. It has practical applications in various fields such as image recognition, natural language processing, fraud detection, and recommendation systems.']\n",
    "sentence2 = 'Machine learning is the process of teaching computers to learn from data, without being explicitly programmed. By analyzing and finding patterns in large datasets, machine learning algorithms can be used to make predictions or decisions in a variety of applications.'\n",
    "score = obj.calculate_similarity(sentence1,sentence2)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 100\n",
      "Score: 100\n",
      "Score: 8\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import euclidean\n",
    "from math import ceil\n",
    "\n",
    "class Nlp_trans_SimCalc:\n",
    "    def __init__(self, model_name='bert-base-nli-mean-tokens'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def calculate_similarity(self, sentences_list, sentence):\n",
    "        sentence_embeddings = self.model.encode([sentence])\n",
    "        sentences_embeddings = self.model.encode(sentences_list)\n",
    "\n",
    "        similarity_scores = []\n",
    "\n",
    "        cos_similarities = util.pytorch_cos_sim(sentence_embeddings, sentences_embeddings).squeeze()\n",
    "        cosine_similarity_scores = [ceil(similarity.item() * 100) if similarity.item() > 0 else 0 for similarity in cos_similarities]\n",
    "\n",
    "        euclidean_distances = [euclidean(sentence_embeddings[0], embedding) for embedding in sentences_embeddings]\n",
    "        euclidean_similarity_scores = [self.calculate_similarity_score(abs(distance)) for distance in euclidean_distances]\n",
    "\n",
    "        for cosine_score, euclidean_score in zip(cosine_similarity_scores, euclidean_similarity_scores):\n",
    "            similarity_scores.append(ceil(0.15 * cosine_score + 0.70 * euclidean_score + 0.15 * cosine_score))\n",
    "\n",
    "        return max(similarity_scores)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_similarity_score(distance):\n",
    "        min_distance = 0.0  # Minimum possible distance\n",
    "        max_distance = 10.0  # Maximum possible distance\n",
    "\n",
    "        normalized_distance = (distance - min_distance) / (max_distance - min_distance)\n",
    "        similarity = 100 * (1 - normalized_distance) * 3.3\n",
    "\n",
    "        return similarity if 0 < similarity < 100 else 100 if similarity > 100 else 0\n",
    "\n",
    "        \n",
    "        \n",
    "# Create an instance of the Nlp_eng_SimCalc class\n",
    "sim_calc = Nlp_trans_SimCalc()\n",
    "\n",
    "# Sample test cases\n",
    "sentence1 = [\"The cat is sitting on the mat.\", \"I like to play tennis.\", \"The sky is blue.\"]\n",
    "sentence2 = \"The cat is on the mat.\"\n",
    "score = sim_calc.calculate_similarity(sentence1, sentence2)\n",
    "print(\"Score:\", score)\n",
    "\n",
    "sentence1 = [\"The cat is sitting on the mat.\", \"I like to play tennis.\", \"The sky is blue.\"]\n",
    "sentence2 = \"I enjoy playing tennis.\"\n",
    "score = sim_calc.calculate_similarity(sentence1, sentence2)\n",
    "print(\"Score:\", score)\n",
    "\n",
    "sentence1 = [\"The cat is sitting on the mat.\", \"I like to play tennis.\", \"The sky is blue.\"]\n",
    "sentence2 = \"The sky is red.\"\n",
    "score = sim_calc.calculate_similarity(sentence1, sentence2)\n",
    "print(\"Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import whisper\n",
    "import torch\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class SpeechRecognizer:\n",
    "    def __init__(self, model_name=\"base\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cpu\"\n",
    "        self.model = self.load_model()\n",
    "\n",
    "        # Set up warnings filter\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "    def banner(self, text):\n",
    "        print(f\"# {text} #\\n\")\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def load_model(self):\n",
    "        return whisper.load_model(self.model_name, device=self.device)\n",
    "\n",
    "    def convert_text(self, audio_file):\n",
    "        self.banner(\"Transcribing texts\")\n",
    "        try:\n",
    "            result = self.model.transcribe(audio_file)\n",
    "            return result[\"text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred during audio file conversion: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import warnings\n",
    "import whisper\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class SpeechRecognizer:\n",
    "    \n",
    "    def __init__(self, model_name=\"base\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = \"cpu\"\n",
    "        self.model = self.load_model()\n",
    "\n",
    "    def banner(self, text):\n",
    "        print(f\"# {text} #\\n\")\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def load_model(self):\n",
    "        return whisper.load_model(self.model_name, device=self.device)\n",
    "\n",
    "    def convert_text(self, audio_file):\n",
    "        warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "        self.banner(\"Transcribing texts\")\n",
    "        result = self.model.transcribe(audio_file)\n",
    "        warnings.resetwarnings()\n",
    "        return result[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before function execution\n",
      "Hello, world!\n",
      "After function execution\n"
     ]
    }
   ],
   "source": [
    "def decorator_func(func):\n",
    "    def wrapper():\n",
    "        print(\"Before function execution\")\n",
    "        func()\n",
    "        print(\"After function execution\")\n",
    "    return wrapper\n",
    "\n",
    "@decorator_func\n",
    "def hello():\n",
    "    print(\"Hello, world!\")\n",
    "\n",
    "hello()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your JSON file\n",
    "json_file_path = r'D:\\Projects and codes\\interview\\resources\\extinsion_interview\\test\\result.json'\n",
    "\n",
    "# Open the JSON file in write mode and truncate its content\n",
    "with open(json_file_path, 'w') as f:\n",
    "    f.truncate(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got the text\n",
      "Review: POSITIVE\n",
      "Data successfully pushed to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from time import sleep\n",
    "import pymongo\n",
    "\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", revision=\"af0f99b\")\n",
    "\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "        result = self.classifier(text, truncation=True)[0]\n",
    "        sentiment = result['label']\n",
    "        score = result['score']\n",
    "        positive_prob = score if sentiment == 'POSITIVE' else 1 - score\n",
    "        negative_prob = 1 - positive_prob\n",
    "\n",
    "        return sentiment, positive_prob, negative_prob\\\n",
    "            \n",
    "    def push_to_Db(self,data):\n",
    "        \n",
    "        try:\n",
    "            # Establish a connection to the MongoDB server\n",
    "            client = pymongo.MongoClient(\"mongodb+srv://webinterview:12345@cluster0.unj3vql.mongodb.net/?retryWrites=true&w=majority\")\n",
    "\n",
    "            # Access the desired database\n",
    "            db = client[\"main\"]\n",
    "\n",
    "            # Access the desired collection\n",
    "            collection = db[\"Review\"]\n",
    "\n",
    "            # Insert the data into the collection\n",
    "            collection.insert_one(data)\n",
    "\n",
    "            print(\"Data successfully pushed to MongoDB.\")\n",
    "        except Exception as error:\n",
    "            print(\"Error while connecting to MongoDB:\", error)\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    analyzer = SentimentAnalyzer()\n",
    "    \n",
    "    file_path = r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\test\\review.txt\"\n",
    "    while True:\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                text = file.read()\n",
    "                if text != \"\":\n",
    "                    print(\"Got the text\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"Waiting for text\")\n",
    "                    sleep(5)\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"File '{file_path}' not found.\")\n",
    "            sleep(5)\n",
    "            \n",
    "        \n",
    "    sentiment, positive_prob, negative_prob = analyzer.analyze_sentiment(text)\n",
    "\n",
    "    data = {\n",
    "        \"sentiment\":sentiment,\n",
    "        \"positive_prob\":positive_prob,\n",
    "        \"negative_prob\":negative_prob,\n",
    "        \"text\":text\n",
    "    }\n",
    "    \n",
    "    print(\"Review:\", sentiment)\n",
    "    # print(\"Positive probability:\", positive_prob)\n",
    "    # print(\"Negative probability:\", negative_prob)\n",
    "    analyzer.push_to_Db(data)\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: NEGATIVE\n",
      "Positive probability: 0.05779218673706055\n",
      "Negative probability: 0.9422078132629395\n"
     ]
    }
   ],
   "source": [
    "analyzer = SentimentAnalyzer()\n",
    "text = \"However, one aspect where XYZ Software falls short is its customer support. While the software itself is exceptional, the response time and assistance from the support team have been subpar. There have been instances where queries took longer than expected to be addressed, which can be frustrating when encountering critical issues. Improving the responsiveness and effectiveness of customer support would greatly enhance the overall user experience.\"\n",
    "sentiment, positive_prob, negative_prob = analyzer.analyze_sentiment(text)\n",
    "\n",
    "print(\"Sentiment:\", sentiment)\n",
    "print(\"Positive probability:\", positive_prob)\n",
    "print(\"Negative probability:\", negative_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detailed': [\"As the capital of France, Paris is the seat of France's national government. For the executive, the two chief officers each have their own official\\xa0...\", 'Paris is the capital of France, the largest country of Europe with 550 000 km2 (65 millions inhabitants). Paris has 2.234 million inhabitants end 2011.', \"The capital and by far the most important city of France is Paris, one of the world's preeminent cultural and commercial centres.\", 'List of capitals of France  Bordeaux (September 1914) The French government was relocated from Paris to Bordeaux very briefly during World War I, when it was\\xa0...', 'Paris, city and capital of France, situated in the north-central part of the country. People were living on the site of the present-day city, located along\\xa0...', \"Oct 4, 2013 ... THIS YEAR MARSEILLE is a European Capital of Culture, so new museums have opened, the streets have been spruced up for tourists, and there's a\\xa0...\", 'Paris is the capital and most populous city of France. Situated on the Seine River, in the north of the country, it is in the centre of the le-de-France\\xa0...', \"Sep 25, 2021 ... GRASSE, France  The town of Grasse sits in the hills above the more famous French Riviera city of Cannes, and it doesn't have the\\xa0...\", \"The correct option is C Paris Paris is the capital of France. It is the most popular city in France. It is often dubbed as the 'Fashion capital of the World'.\", 'What is the Capital of France? Paris is the capital city of France and the center of France. It is built on the Seine River, in the middle of the Paris Basin.'], 'brief': [], 'one_line': []}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "def generate_answers(question, api_key=\"AIzaSyAni60XCniMwfWiU3ZVHWX2TCFSgDf3N9M\", cx=\"87e385fa22560468e\"):\n",
    "\n",
    "    # Create the search request.\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cx,\n",
    "        'q': question,\n",
    "        'num': 10,\n",
    "        'safe': 'off',\n",
    "    }\n",
    "    url = 'https://www.googleapis.com/customsearch/v1?' + urllib.parse.urlencode(params)\n",
    "\n",
    "    # Make the search request.\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check the response status code.\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Error searching Google: {}'.format(response.status_code))\n",
    "\n",
    "    # Parse the response JSON.\n",
    "    results = json.loads(response.content)\n",
    "\n",
    "    # Extract and categorize the answers.\n",
    "    categorized_answers = {\n",
    "        'detailed': [],\n",
    "        'brief': [],\n",
    "        'one_line': []\n",
    "    }\n",
    "    for result in results['items']:\n",
    "        answer = result['snippet']\n",
    "        if len(answer.split()) >= 20:\n",
    "            categorized_answers['detailed'].append(answer)\n",
    "        elif len(answer.split()) >= 5:\n",
    "            categorized_answers['brief'].append(answer)\n",
    "        else:\n",
    "            categorized_answers['one_line'].append(answer)\n",
    "\n",
    "    return categorized_answers\n",
    "\n",
    "\n",
    "# Example usage\n",
    "question = \"What is the capital of France?\"\n",
    "api_key = \"AIzaSyAni60XCniMwfWiU3ZVHWX2TCFSgDf3N9M\"\n",
    "cx = \"87e385fa22560468e\"\n",
    "answers = generate_answers(question)\n",
    "print(answers)\n",
    "# # Print the retrieved answers\n",
    "# print(\"Detailed answers:\")\n",
    "# for answer in answers['detailed']:\n",
    "#     print(\"- \", answer)\n",
    "\n",
    "# print(\"\\nBrief answers:\")\n",
    "# for answer in answers['brief']:\n",
    "#     print(\"- \", answer)\n",
    "\n",
    "# print(\"\\nOne-line answers:\")\n",
    "# for answer in answers['one_line']:\n",
    "#     print(\"- \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects and codes\\interview\\resources\\extinsion_interview\\test\\test.ipynb Cell 40\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/extinsion_interview/test/test.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m answers \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/extinsion_interview/test/test.ipynb#X54sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m questions:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/extinsion_interview/test/test.ipynb#X54sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     answer \u001b[39m=\u001b[39m nlp(question\u001b[39m=\u001b[39mquestion, context\u001b[39m=\u001b[39mtext)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/extinsion_interview/test/test.ipynb#X54sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     answers\u001b[39m.\u001b[39mappend(answer)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects%20and%20codes/interview/resources/extinsion_interview/test/test.ipynb#X54sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Print the answers\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the question-answering model\n",
    "nlp = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# Example questions\n",
    "questions = [\n",
    "    \"What is supervised learning?\",\n",
    "    \"What are decision trees and how are they used in machine learning?\",\n",
    "    \"What is clustering and how is it used in machine learning?\",\n",
    "    \"What is an operating system and what are its main functions?\",\n",
    "    \"What is a training set and its importance in machine learning?\",\n",
    "    \"What is a test set and its role in evaluating machine learning models?\",\n",
    "]\n",
    "\n",
    "# Get answers for each question\n",
    "answers = []\n",
    "for question in questions:\n",
    "    answer = nlp(question=question, context=text)\n",
    "    answers.append(answer)\n",
    "\n",
    "# Print the answers\n",
    "for i, answer in enumerate(answers):\n",
    "    print(f\"Question: {questions[i]}\")\n",
    "    print(f\"Answer: {answer['answer']}\")\n",
    "    print(f\"Score: {answer['score']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb+srv://webinterview:12345@cluster0.unj3vql.mongodb.net/?retryWrites=true&w=majority\")\n",
    "db = client[\"main\"]\n",
    "collection = db[\"questions\"]\n",
    "\n",
    "# Read data from text file\n",
    "# Read data from text file\n",
    "with open(r\"D:\\Projects and codes\\interview\\resources\\extinsion_interview\\test\\review.txt\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "# Process each line and insert into the database\n",
    "# Process each question and answers and insert into the database\n",
    "for i in range(0, len(lines), 6):\n",
    "    question = lines[i].strip().strip('\"')\n",
    "    answer1 = lines[i+1].strip().split(\": \")[1].strip('\"')\n",
    "    answer2 = lines[i+2].strip().split(\": \")[1].strip('\"')\n",
    "    answer3 = lines[i+3].strip().split(\": \")[1].strip('\"')\n",
    "    answer4 = lines[i+4].strip().split(\": \")[1].strip('\"')\n",
    "    answer5 = lines[i+5].strip().split(\": \")[1].strip('\"')\n",
    "\n",
    "    document = {\n",
    "        \"question\": question,\n",
    "        \"answer1\": answer1,\n",
    "        \"answer2\": answer2,\n",
    "        \"answer3\": answer3,\n",
    "        \"answer4\": answer4,\n",
    "        \"answer5\": answer5\n",
    "    }\n",
    "\n",
    "    # Insert the document into the database\n",
    "    collection.insert_one(document)\n",
    "\n",
    "# Close the database connection\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
